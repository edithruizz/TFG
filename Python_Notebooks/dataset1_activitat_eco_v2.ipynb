{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ic6Wp5lEcIK"
      },
      "source": [
        "## Improving Water Consumption Management in Barcelona through Data Quality Enhancement and Prediction Models\n",
        "#### **TFG 2023-2024**\n",
        "#### **Author: Edith Ruiz Macià**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgArp49QG7dA"
      },
      "source": [
        "### Analysis of dataset Dataset1_activitat_eco_v2.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXSbev31DSda"
      },
      "source": [
        "### **About the Notebook**\n",
        "\n",
        "The aim of this notebook (.ipynb) is to analyze and modify the dataset dataset1_activitat_eco_v2.csv provided by Aigües de Barcelona in order to improve the quality of the data and learn its insights through its visualization.\n",
        "\n",
        "The code is structured in different levels.\n",
        "\n",
        "1. Given the database, we start with an Exploratory Data Analysis that will provide us a general knowledge of how the raw data looks like.\n",
        "2. Then, an intensive data cleaning will be applied which will transform the data and handle null, negative, and outlier values.\n",
        "3. We will enhance the data by adding columns that will add up to the understanding of the data and also, by combining outside data that might be relevant to ours.\n",
        "4. In order to have a complete dataset we will make a projection of absent or incorrect values previously detected in section 2. To do so, we will test several algorithms and see with which one we obtain better results.\n",
        "5. Anomalies that were previously discarded and substituted by projected values with our predictive model must also be taken into account and analyzed. That is why we will classify and visualize them.\n",
        "\n",
        "Finally, the updated resulting database can be downloaded in order to be used in other programs or other purposes of the user."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y22J26JOFGf2"
      },
      "source": [
        "#### ***Execution time***\n",
        "bla bla bla\n",
        "\n",
        "#### ***How to execute the code***\n",
        "This is a Python Notebook, so the code should be run either cell by cell or by running all the notebook at once. At the beginning, review and make sure the paths to all datasets are correct for your folders' distribution in order for everything to run smoothly.\n",
        "\n",
        "\n",
        "#### ***Libraries and packages used in the project***\n",
        "bla bla bla"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XbMgh4p6HYDV",
        "outputId": "4d8f189f-3db1-4394-979a-5fd48aed9c95"
      },
      "outputs": [],
      "source": [
        "# In case you are working in google colab you can mount your google drive account here\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsqyRqFD0Jr9"
      },
      "source": [
        "## 0. Importing datasets and libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DHWV24mK0N_R"
      },
      "outputs": [],
      "source": [
        "# Import the necessary libraries\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import xgboost as xgb\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.svm import SVR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "9TzKiaam0Pdz",
        "outputId": "8b585030-4298-42d3-f29b-7dc8c12f1f9c"
      },
      "outputs": [],
      "source": [
        "# Change your path of the data if necessary\n",
        "dataset_1_act_eco = 'C:/Users/edith/Desktop/TFG/Datasets/Aigües de Barcelona/Sets-de-dades/dataset1_activitat_eco_v2.csv'\n",
        "dataset_1_act_eco = pd.read_csv(dataset_1_act_eco)\n",
        "dataset_1_act_eco.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X28CTYqz0Ze3"
      },
      "source": [
        "## 1. Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As a starting point, we need to explore the data. We analyze the summary of descriptive statistics and plots for each of the datasets in order to detect the corresponding anomalies and subsequently process them correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qfHM9dJa0XwY",
        "outputId": "7552a87a-64c1-4b32-fd80-b79d828e902d"
      },
      "outputs": [],
      "source": [
        "dataset_1_act_eco.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "TOZJw42i0fgu",
        "outputId": "373ded53-acf1-4809-9678-7560299686b7"
      },
      "outputs": [],
      "source": [
        "dataset_1_act_eco.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fCT_K3IY0h4A"
      },
      "outputs": [],
      "source": [
        "# Accumulated Consumption over Time\n",
        "dataset_1_act_eco['Data/Fecha/Date'] = pd.to_datetime(dataset_1_act_eco['Data/Fecha/Date'])\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(dataset_1_act_eco['Data/Fecha/Date'], dataset_1_act_eco['Consum acumulat (L/dia)/Consumo acumulado (L/día)/Accumulated Consumption (L/day)'], color='blue', marker='o', s=50, alpha=0.7, label='Data Points')\n",
        "plt.xlabel('Date', fontsize=12)\n",
        "plt.ylabel('Accumulated Consumption (L/day)', fontsize=12)\n",
        "plt.title('Time Series Plot of Accumulated Consumption', fontsize=15)\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Aggregate total consumption for each municipality\n",
        "total_consumption_per_municipality = dataset_1_act_eco.groupby('Municipi/Municipio/Municipality')['Consum acumulat (L/dia)/Consumo acumulado (L/día)/Accumulated Consumption (L/day)'].sum()\n",
        "total_consumption_per_municipality.plot(kind='bar', figsize=(10,6))\n",
        "plt.xlabel('Municipality', fontsize=12)\n",
        "plt.ylabel('Total Accumulated Consumption (L/day)', fontsize=12)\n",
        "plt.title('Total Accumulated Consumption per Municipality', fontsize=15)\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Aggregate total consumption for each type of use\n",
        "total_consumption_per_use = dataset_1_act_eco.groupby('Ús/Uso/Use')['Consum acumulat (L/dia)/Consumo acumulado (L/día)/Accumulated Consumption (L/day)'].sum()\n",
        "plt.figure(figsize=(10, 6))\n",
        "total_consumption_per_use.plot(kind='pie', autopct='%1.1f%%', startangle=140, colors=['lightskyblue', 'blue', 'cornflowerblue'])\n",
        "plt.title('Total Accumulated Consumption per Type of Use', fontsize=14)\n",
        "plt.ylabel(None)\n",
        "plt.tight_layout()\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hO80Filo0iPp"
      },
      "source": [
        "**Insights:**\n",
        "- Negative consumption values are considered as incorrect values for the Cumulative Consumption (L/day) column. Consumption has to be positive.\n",
        "- The number of meters ('Number of meters') varies greatly, with some locations having just one meter and others up to 394 meters.\n",
        "- In the scatter plot we can clearly see outliers that will be removed later on.\n",
        "- Most of the consumption is accumulated in the Municipality of Barcelona and it's of Industrial use."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_d_LlHAd0olY"
      },
      "source": [
        "## 2. Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWhww6j50p9l"
      },
      "source": [
        "##### **2.1. Data Transformation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l5z3wQ2h0rmj"
      },
      "outputs": [],
      "source": [
        "# Rename dataset_1_activitat_eco_v2 columns\n",
        "new_column_names = {\n",
        "    'Districte/Distrito/District': 'District',\n",
        "    'Codi postal/Código postal/Postcode': 'Postcode',\n",
        "    'Municipi/Municipio/Municipality': 'Municipality',\n",
        "    'Data/Fecha/Date': 'Date',\n",
        "    'Ús/Uso/Use': 'Use',\n",
        "    \"Tipus d'activitat econòmica/Tipo de actividad económica/Type of economic activity\": 'Type of economic activity',\n",
        "    'Nombre de comptadors/Número de contadores/Number of meters': 'Number of meters',\n",
        "    'Consum acumulat (L/dia)/Consumo acumulado (L/día)/Accumulated Consumption (L/day)': 'Accumulated Consumption (L/day)'\n",
        "}\n",
        "\n",
        "dataset_1_act_eco.rename(columns=new_column_names, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Randomly sample 700000 data points\n",
        "sampled_indices = np.random.choice(len(dataset_1_act_eco), size=700000, replace=False)\n",
        "sampled_data = dataset_1_act_eco.iloc[sampled_indices]\n",
        "\n",
        "# Scatter Plot for Accumulated Consumption vs Number of Meters without negatives\n",
        "plt.scatter(sampled_data['Number of meters'], sampled_data['Accumulated Consumption (L/day)'])\n",
        "plt.title('Accumulated Consumption vs Number of Meters without negatives')\n",
        "plt.xlabel('Number of Meters')\n",
        "plt.ylabel('Accumulated Consumption (L/day)')\n",
        "plt.ylim(0, 100000)\n",
        "\n",
        "# Calculate the trend line\n",
        "x = sampled_data['Number of meters']\n",
        "y = sampled_data['Accumulated Consumption (L/day)']\n",
        "m, b = np.polyfit(x, y, 1)\n",
        "plt.plot(x, m*x + b, color='red')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We cannot assure that consumption is not normalized, but according to the plots above it does not look like it therefore, they have to be normalized. We will divide the Accumulated Consumption by the Number of Meters in order to obtain the Accumulated Consumption for a single Meter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Normalization of Accumulated Consumption (L/day) by Number of meters\n",
        "dataset_1_act_eco['Normalized Accumulated Consumption (L/day)'] = round(dataset_1_act_eco['Accumulated Consumption (L/day)'] / dataset_1_act_eco['Number of meters'], 3)\n",
        "dataset_1_act_eco.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Distribution of normalized vs non normalized\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.scatter(dataset_1_act_eco[\"Date\"], dataset_1_act_eco[\"Accumulated Consumption (L/day)\"], color='blue',  marker='o', s=20, alpha=0.7, label='Accumulated Consumption')\n",
        "plt.scatter(dataset_1_act_eco[\"Date\"], dataset_1_act_eco[\"Normalized Accumulated Consumption (L/day)\"], color='lightskyblue',  marker='o', s=20, alpha=0.7, label='Normalized Accumulated Consumption')\n",
        "plt.title(\"Distribution of Accumulated Consumption vs Normalized Accumulated Consumption over Time\", fontsize=12)\n",
        "plt.xlabel(\"Date\", fontsize=10)\n",
        "plt.ylabel(\"Accumulated Consumption (L/day)\", fontsize=10)\n",
        "plt.xticks(rotation=45, ha='right', fontsize=8)\n",
        "plt.yticks(fontsize=8)\n",
        "plt.grid(True)\n",
        "plt.legend(fontsize=7)\n",
        "tick_frequency = 100\n",
        "plt.gca().xaxis.set_major_locator(mdates.DayLocator(interval=tick_frequency))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "koW6fKMV02ER",
        "outputId": "3db154d0-0d84-4423-97da-3278dbfb7245"
      },
      "outputs": [],
      "source": [
        "dataset_1_act_eco.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oD1YGPSq060b"
      },
      "source": [
        "##### **2.2. Null Values**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FigpvFQ707Qg",
        "outputId": "3162311f-db5c-4a1a-d83b-3e8aa94a6eb3"
      },
      "outputs": [],
      "source": [
        "# Null values per column\n",
        "columns_dataset_1_act_eco = dataset_1_act_eco.columns\n",
        "for column in columns_dataset_1_act_eco:\n",
        "  print(\"Column:\", column, \"- Null values: \", dataset_1_act_eco[column].isnull().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### **2.3. Wrong Values**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we want to identify the erroneous values in our data. We consider as wrong values the negative consumptions. We will store those values for later analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# We identify negative values in consumption\n",
        "num_negative_consum_rows = len(dataset_1_act_eco[dataset_1_act_eco['Normalized Accumulated Consumption (L/day)'] < 0])\n",
        "print(\"Negative in 'Accumulated Consumption (L/day)' in the dataset:\", num_negative_consum_rows)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Copy the negative values into the anomalies dataset for later analysis\n",
        "anomalies = dataset_1_act_eco[dataset_1_act_eco['Normalized Accumulated Consumption (L/day)'] < 0].copy()\n",
        "anomalies.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Replace negative values in Normalized Accumulated Consumption (L/day) by null\n",
        "dataset_1_act_eco_1 = dataset_1_act_eco.copy()\n",
        "dataset_1_act_eco_1.loc[dataset_1_act_eco_1['Normalized Accumulated Consumption (L/day)'] < 0, 'Normalized Accumulated Consumption (L/day)'] = np.nan\n",
        "\n",
        "# Let's put null also the values of the Accumulated Consumption (L/day)\n",
        "dataset_1_act_eco_1.loc[dataset_1_act_eco_1['Accumulated Consumption (L/day)'] < 0, 'Accumulated Consumption (L/day)'] = np.nan\n",
        "\n",
        "# Number of negative values in Normalized Accumulated Consumption (L/day) after removing negative values\n",
        "num_negative_consum_rows2 = len(dataset_1_act_eco_1[dataset_1_act_eco_1['Normalized Accumulated Consumption (L/day)'] < 0])\n",
        "print(\"Negative in 'Normalized Accumulated Consumption (L/day)' in the dataset:\", num_negative_consum_rows2)\n",
        "\n",
        "# Number of null values in Normalized Accumulated Consumption (L/day) after removing negative values\n",
        "print(\"Number of Nulls in the dataset without negative values:\", dataset_1_act_eco_1['Normalized Accumulated Consumption (L/day)'].isnull().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suRy20Ir1Fwv"
      },
      "source": [
        "##### **2.4. Outliers**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "EBR1nsBP1GY_",
        "outputId": "93b835ad-99ea-46d0-dbd7-4005d41278f2"
      },
      "outputs": [],
      "source": [
        "# Treatment of outliers in Normalized Accumulated Consumption (L/day) with the IQR method\n",
        "\n",
        "# We separate according to USE: Industrial, Domestic or Commercial\n",
        "domestic_df = dataset_1_act_eco_1[dataset_1_act_eco_1['Use'].str.contains('Domèstic', case=False, na=False)].copy()\n",
        "industrial_df = dataset_1_act_eco_1[dataset_1_act_eco_1['Use'].str.contains('Industrial', case=False, na=False)].copy()\n",
        "comercial_df = dataset_1_act_eco_1[dataset_1_act_eco_1['Use'].str.contains('Comercial', case=False, na=False)].copy()\n",
        "\n",
        "def outliers_iqr(dataframe):\n",
        "    global anomalies\n",
        "\n",
        "    # We compute the IQR for the 'Consumption' column\n",
        "    consum_col = 'Normalized Accumulated Consumption (L/day)'\n",
        "    Q1 = dataframe[consum_col].quantile(0.25)\n",
        "    Q3 = dataframe[consum_col].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "\n",
        "    # We identify the outliers\n",
        "    outlier_filter = ((dataframe[consum_col] < (Q1 - 1.5 * IQR)) | (dataframe[consum_col] > (Q3 + 1.5 * IQR)))\n",
        "\n",
        "    # Copy the outlier value into the anomalies dataset for later analysis\n",
        "    outliers = dataframe[outlier_filter].copy()\n",
        "    anomalies = pd.concat([anomalies, outliers], ignore_index=True)\n",
        "\n",
        "    # And replace them with null\n",
        "    dataframe.loc[outlier_filter, consum_col] = np.nan\n",
        "\n",
        "    return dataframe\n",
        "\n",
        "# We apply the function for each dataframe corresponding to each \"Use\"\n",
        "domestic_df = outliers_iqr(domestic_df)\n",
        "industrial_df = outliers_iqr(industrial_df)\n",
        "comercial_df = outliers_iqr(comercial_df)\n",
        "\n",
        "dataset_1_act_eco_filtered = pd.concat([domestic_df, industrial_df, comercial_df], ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let's put to null also the values that correspond to those outliers in 'Accumulated Consumption (L/day)'\n",
        "dataset_1_act_eco_filtered.loc[dataset_1_act_eco_filtered['Normalized Accumulated Consumption (L/day)'].isnull(), 'Accumulated Consumption (L/day)'] = np.nan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "anomalies.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Number of null values in Normalized Accumulated Consumption (L/day) prior to removing outliers\n",
        "print(\"Number of Nulls in the dataset with outliers but no negatives:\", dataset_1_act_eco_1['Normalized Accumulated Consumption (L/day)'].isnull().sum())        #dataset with outliers\n",
        "\n",
        "# Number of null values in Normalized Accumulated Consumption (L/day) after removing outliers\n",
        "print(\"Number of Nulls in the dataset without outliers nor negatives:\", dataset_1_act_eco_filtered['Normalized Accumulated Consumption (L/day)'].isnull().sum())   #dataset without outliers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HbBUCyy1sd3"
      },
      "source": [
        "##### **2.5. Storage of Anomalies**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If all anomalies in the data have been cleaned correctly we should have the sum of the number of outlier values and negatives remaining in our anomalies stored.\n",
        "\n",
        "Negative values removed = 32591\n",
        "\n",
        "Outliers removed = 2373293 - 32591 = 2340702\n",
        "\n",
        "Total number of anomalies = 2373293"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xcvdx5K91tGI"
      },
      "outputs": [],
      "source": [
        "# Save current clean dataset for classification of anomalies\n",
        "clean_data = dataset_1_act_eco_filtered.copy()\n",
        "\n",
        "anomalies.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9iqYyZIP1xJ0"
      },
      "source": [
        "## 3. Data Enhancement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SC3pUbbU1yu6"
      },
      "outputs": [],
      "source": [
        "# Convert \"Date\" column to datetime\n",
        "dataset_1_act_eco_filtered['Date'] = pd.to_datetime(dataset_1_act_eco_filtered['Date'])\n",
        "dataset_1_act_eco_filtered.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "B_ZXKaIP2B9c",
        "outputId": "0e46d021-324f-45d3-dce1-dceac5310d0c"
      },
      "outputs": [],
      "source": [
        "# We create a new \"Season\" column based on the \"Date\" column\n",
        "def map_to_season(month):\n",
        "    if 3 <= month <= 5:\n",
        "        return 'Spring'\n",
        "    elif 6 <= month <= 8:\n",
        "        return 'Summer'\n",
        "    elif 9 <= month <= 11:\n",
        "        return 'Autumn'\n",
        "    else:\n",
        "        return 'Winter'\n",
        "\n",
        "dataset_1_act_eco_filtered['Season'] = dataset_1_act_eco_filtered['Date'].dt.month.map(map_to_season)\n",
        "\n",
        "# We create a new column \"Day_of_Week\" based on the column \"Date\"\n",
        "dataset_1_act_eco_filtered['Day of Week'] = dataset_1_act_eco_filtered['Date'].dt.day_name()\n",
        "\n",
        "# We create a new column \"Month\" based on the column \"Date\"\n",
        "dataset_1_act_eco_filtered['Month'] = dataset_1_act_eco_filtered['Date'].dt.month\n",
        "\n",
        "# We create a new column \"Year\" based on the column \"Date\"\n",
        "dataset_1_act_eco_filtered['Year'] = dataset_1_act_eco_filtered['Date'].dt.year\n",
        "\n",
        "dataset_1_act_eco_filtered.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we can see now all data in this dataset ranges from years 2019 to 2022. This is important as now we are going to add new information from other datasets and we are going to need the dates to match."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJWDyvMCuSM_",
        "outputId": "531cccbe-7923-446c-fbdf-62f2d0ca6b50"
      },
      "outputs": [],
      "source": [
        "dataset_1_act_eco_filtered['Year'].unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets add information to the dataset about the precipitations in Barcelona for the dates of each entrie as well as information about the temperatures. Both precipitation and temperatures should have a strong correlation with the consumed water."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Change your path of the data if necessary\n",
        "dataset_rain = 'C:/Users/edith/Desktop/TFG/Datasets/Geotemporals/precipitacionsbcndesde1786_2023_long.csv'\n",
        "dataset_rain = pd.read_csv(dataset_rain)\n",
        "dataset_rain.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Filter the dataset to keep only the desired years\n",
        "years_to_keep = [2019, 2020, 2021, 2022]\n",
        "dataset_rain = dataset_rain[dataset_rain['Any'].isin(years_to_keep)]\n",
        "\n",
        "# Display the first few rows of the filtered dataset\n",
        "dataset_rain.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset_1_act_eco_filtered.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def map_to_precipitation(dataset_filtered, dataset_rain):\n",
        "\n",
        "    # Merge datasets on 'Year' and 'Month'\n",
        "    merged_data = pd.merge(dataset_filtered, dataset_rain, left_on=['Year', 'Month'], right_on=['Any', 'Mes'], how='left')\n",
        "\n",
        "    # Drop unnecessary columns from the merged dataset\n",
        "    merged_data.drop(['Any', 'Mes', 'Desc_Mes'], axis=1, inplace=True)\n",
        "\n",
        "    return merged_data\n",
        "\n",
        "half = len(dataset_1_act_eco_filtered) // 2\n",
        "first_half = dataset_1_act_eco_filtered.iloc[:half]\n",
        "second_half = dataset_1_act_eco_filtered.iloc[half:]\n",
        "\n",
        "first_half = map_to_precipitation(dataset_1_act_eco_filtered, dataset_rain)\n",
        "second_half = map_to_precipitation(dataset_1_act_eco_filtered, dataset_rain)\n",
        "dataset_1_act_eco_filtered = pd.concat([first_half, second_half])\n",
        "dataset_1_act_eco_filtered.rename(columns = {'Precipitacions': 'Precipitations'}, inplace=True)\n",
        "dataset_1_act_eco_filtered.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Change your path of the data if necessary\n",
        "dataset_temp = 'C:/Users/edith/Desktop/TFG/Datasets/Geotemporals/temperaturesbcndesde1780_2023_long.csv'\n",
        "dataset_temp = pd.read_csv(dataset_temp)\n",
        "dataset_temp.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Filter the dataset to keep only the desired years\n",
        "years_to_keep = [2019, 2020, 2021, 2022]\n",
        "dataset_temp = dataset_temp[dataset_temp['Any'].isin(years_to_keep)]\n",
        "\n",
        "# Display the first few rows of the filtered dataset\n",
        "dataset_temp.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def map_to_temperature(dataset_filtered, dataset_temp):\n",
        "\n",
        "    # Merge datasets on 'Year' and 'Month'\n",
        "    merged_data = pd.merge(dataset_filtered, dataset_temp, left_on=['Year', 'Month'], right_on=['Any', 'Mes'], how='left')\n",
        "\n",
        "    # Drop unnecessary columns from the merged dataset\n",
        "    merged_data.drop(['Any', 'Mes', 'Desc_Mes'], axis=1, inplace=True)\n",
        "\n",
        "    return merged_data\n",
        "\n",
        "half = len(dataset_1_act_eco_filtered) // 2\n",
        "first_half = dataset_1_act_eco_filtered.iloc[:half]\n",
        "second_half = dataset_1_act_eco_filtered.iloc[half:]\n",
        "\n",
        "first_half = map_to_temperature(dataset_1_act_eco_filtered, dataset_rain)\n",
        "second_half = map_to_temperature(dataset_1_act_eco_filtered, dataset_rain)\n",
        "dataset_1_act_eco_filtered = pd.concat([first_half, second_half])\n",
        "dataset_1_act_eco_filtered.rename(columns = {'Temperatura': 'Temperature'}, inplace=True)\n",
        "dataset_1_act_eco_filtered.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKtTLZH-2dRK"
      },
      "source": [
        "## 4. Data Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In order to do the projection of incorrect values (negative and outliers) that right now happen to be null values in the dataset, we will implement several data prediction models, compare their results and conclude which gives the best results for our data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will now encode the variables that aren't numerical in order to work with them and plot a correlation matrix to see which features explain best our target variable, Accumulated Consumption (L/day)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pmk3Aox32rQF"
      },
      "outputs": [],
      "source": [
        "# Convert \"Date\" column to object for label encoding\n",
        "dataset_1_act_eco_filtered['Date'] = dataset_1_act_eco_filtered['Date'].astype(object)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LrsCyySU2vh7"
      },
      "outputs": [],
      "source": [
        "# Label Encoder\n",
        "\n",
        "# Initialize LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "half = len(dataset_1_act_eco_filtered) // 2\n",
        "first_half = dataset_1_act_eco_filtered.iloc[:half]\n",
        "second_half = dataset_1_act_eco_filtered.iloc[half:]\n",
        "\n",
        "# Apply LabelEncoder to each column containing categorical data\n",
        "for column in first_half.columns:\n",
        "    if first_half[column].dtype == 'object':\n",
        "        first_half[column] = label_encoder.fit_transform(first_half[column])\n",
        "\n",
        "for column in second_half.columns:\n",
        "    if second_half[column].dtype == 'object':\n",
        "        second_half[column] = label_encoder.fit_transform(second_half[column])\n",
        "\n",
        "dataset_1_act_eco_filtered = pd.concat([first_half, second_half])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3KNj2pdF24D1"
      },
      "outputs": [],
      "source": [
        "# We remove the null values in to see the correct correlation of the data\n",
        "dataset_1_act_eco_filtered = dataset_1_act_eco_filtered.dropna(subset = ['District'])\n",
        "\n",
        "dataset_filtered_not_null = dataset_1_act_eco_filtered[~dataset_1_act_eco_filtered['Accumulated Consumption (L/day)'].isnull()]\n",
        "corr = dataset_filtered_not_null.corr()\n",
        "plt.figure(figsize=(16,6))\n",
        "heatmap = sns.heatmap(corr, vmin=-1, vmax=1, annot=True, cmap='coolwarm')\n",
        "heatmap.set_title('Correlation Heatmap', fontdict={'fontsize': 12})\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SKs31ymZ2-jF"
      },
      "outputs": [],
      "source": [
        "# Model Training\n",
        "\n",
        "# We remove the null values in the consumption column to train the model\n",
        "dataset_1_act_eco_filtered2 = dataset_1_act_eco_filtered.dropna(subset=['Accumulated Consumption (L/day)'])\n",
        "\n",
        "# Independent variables\n",
        "features = ['Use', 'Number of meters', 'Postcode', 'Municipality', 'District'] # 0.7568\n",
        "# features = ['Number of meters', 'Use', 'Type of economic activity', 'Postcode', 'Municipality', 'District', 'Day of Week', 'Precipitations', 'Temperature']\n",
        "\n",
        "X = dataset_1_act_eco_filtered2[features]\n",
        "\n",
        "# Target variable (dependent variable)\n",
        "y = dataset_1_act_eco_filtered2['Accumulated Consumption (L/day)']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# We make sure that X and Y have the same length for both train and test\n",
        "print(\"Lenght X_train:\", len(X_train))\n",
        "print(\"Length y_train:\", len(y_train))\n",
        "print(\"Lenght X_test:\", len(X_test))\n",
        "print(\"Length y_test:\", len(y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### **4.1. Linear Regression**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "acpbOglj3F2x"
      },
      "outputs": [],
      "source": [
        "# Linear Regression\n",
        "lr_model = LinearRegression()\n",
        "lr_model.fit(X_train, y_train)\n",
        "y_pred = lr_model.predict(X_test)\n",
        "\n",
        "# Evalute the model\n",
        "mse_lr = mean_squared_error(y_test, y_pred)\n",
        "mae_lr = mean_absolute_error(y_test, y_pred)\n",
        "r2_lr = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Linear Regression Mean Squared Error (MSE): {round(np.sqrt(mse_lr), 4)}\")\n",
        "print(f\"Linear Regression Mean Absolute Error (MAE): {round(mae_lr, 4)}\")\n",
        "print(f\"Linear Regression R-squared (R2): {round(r2_lr, 4)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdeoGdTu3jsM"
      },
      "source": [
        "##### **4.2. Random Forest**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model Training\n",
        "\n",
        "# We remove the null values in the consumption column to train the model\n",
        "dataset_1_act_eco_filtered2 = dataset_1_act_eco_filtered.dropna(subset=['Accumulated Consumption (L/day)'])\n",
        "\n",
        "# Reduce the working dataset\n",
        "quarter_rows = len(dataset_1_act_eco_filtered2) // 25\n",
        "dataset_1_act_eco_filtered2 = dataset_1_act_eco_filtered2.iloc[:quarter_rows]\n",
        "\n",
        "# Independent variables\n",
        "features = ['Number of meters', 'Use', 'Date']\n",
        "X = dataset_1_act_eco_filtered2[features]\n",
        "\n",
        "# Target variable (dependent variable)\n",
        "y = dataset_1_act_eco_filtered2['Accumulated Consumption (L/day)']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# We make sure that X and Y have the same length for both train and test\n",
        "print(\"Lenght X_train:\", len(X_train))\n",
        "print(\"Length y_train:\", len(y_train))\n",
        "print(\"Lenght X_test:\", len(X_test))\n",
        "print(\"Length y_test:\", len(y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C1YcXpOw3lMC"
      },
      "outputs": [],
      "source": [
        "# Random Forest\n",
        "rf_model = RandomForestRegressor(random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
        "mae_rf = mean_absolute_error(y_test, y_pred_rf)\n",
        "r2_rf = r2_score(y_test, y_pred_rf)\n",
        "\n",
        "print(f\"Random Forest Mean Squared Error (MSE):\", round(np.sqrt(mse_rf), 4))\n",
        "print(f\"Random Forest Mean Absolute Error (MAE): {round(mae_rf, 4)}\")\n",
        "print(f\"Random Forest R-squared (R2):\", round(r2_rf, 4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sX4h2uv83lhF"
      },
      "source": [
        "##### **4.3. XGBoost**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AURJsbJg3nXV"
      },
      "outputs": [],
      "source": [
        "# XGBoost\n",
        "xgb_model = xgb.XGBRegressor(random_state=42)\n",
        "xgb_model.fit(X_train, y_train)\n",
        "y_pred_xgb = xgb_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
        "mae_xgb = mean_absolute_error(y_test, y_pred_xgb)\n",
        "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
        "\n",
        "print(f\"XGBoost Mean Squared Error (MSE):\", round(np.sqrt(mse_xgb), 4))\n",
        "print(f\"XGBoost Mean Absolute Error (MAE): {round(mae_xgb, 4)}\")\n",
        "print(f\"XGBoost R-squared (R2):\", round(r2_xgb, 4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcyWS1Xl3p3k"
      },
      "source": [
        "##### **4.4. K-Nearest Neighbors**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SqcTEeZv3rPh"
      },
      "outputs": [],
      "source": [
        "# K-Nearest Neighbors\n",
        "knn_model = KNeighborsRegressor()\n",
        "knn_model.fit(X_train, y_train)\n",
        "y_pred_knn = knn_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse_knn = mean_squared_error(y_test, y_pred_knn)\n",
        "mae_knn = mean_absolute_error(y_test, y_pred_knn)\n",
        "r2_knn = r2_score(y_test, y_pred_knn)\n",
        "\n",
        "print(f\"K-Nearest Neighbors Mean Squared Error (MSE):\", round(np.sqrt(mse_knn), 4))\n",
        "print(f\"K-Nearest Neighbors Mean Absolute Error (MAE): {round(mae_knn, 4)}\")\n",
        "print(f\"K-Nearest Neighbors R-squared (R2):\", round(r2_knn, 4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### **4.5. Support Vector Machine**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Support Vector Machine\n",
        "svm_model = SVR()\n",
        "svm_model.fit(X_train, y_train)\n",
        "y_pred_svm = svm_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse_svm = mean_squared_error(y_test, y_pred_svm)\n",
        "mae_svm = mean_absolute_error(y_test, y_pred_svm)\n",
        "r2_svm = r2_score(y_test, y_pred_svm)\n",
        "\n",
        "print(f\"Support Vector Machine Mean Squared Error (MSE):\", round(np.sqrt(mse_svm), 4))\n",
        "print(f\"Support Vector Machine Mean Absolute Error (MAE): {round(mae_svm, 4)}\")\n",
        "print(f\"Support Vector Machine R-squared (R2):\", round(r2_svm, 4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVb8gD593sy-"
      },
      "source": [
        "##### **4.6. Conclusions on Data Prediction Algorithms**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Based on the R-squared values obtained per each model I conclude that ??? is the most accurate for dataset1_v2.csv data. It is considered a good model if the R-squared value is higher than 0.9. Therefore now we will replace the missing values with the predictions of this model. We have already trained model so now we replace all null values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ZTEC5J63upg"
      },
      "outputs": [],
      "source": [
        "# Identifiquem les files amb valor Null en 'Normalized Consum acumulat (L/dia)'\n",
        "missing_rows = dataset_1_act_eco_filtered[dataset_1_act_eco_filtered['Accumulated Consumption (L/day)'].isnull()]\n",
        "\n",
        "features2 = ['Time', 'Use', 'Type of economic activity']\n",
        "X_ = missing_rows[features2]\n",
        "\n",
        "# Predim els valors amb el model\n",
        "predicted_values = model.predict(X_)\n",
        "\n",
        "dataset_1_act_eco_filtered.loc[missing_rows.index, 'Accumulated Consumption (L/day)'] = predicted_values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now check that no null values nor negatives are in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Number of null values in Accumulated Consumption (L/day)\n",
        "print(\"Number of Nulls in the dataset:\", dataset_1_act_eco_filtered['Accumulated Consumption (L/day)'].isnull().sum())\n",
        "\n",
        "# Number of negatives values in Accumulated Consumption (L/day)\n",
        "print(\"Number of Negatives in the dataset:\", len(dataset_1_act_eco_filtered[dataset_1_act_eco_filtered['Accumulated Consumption (L/day)'] < 0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In order to keep working with our normalized values lets once again put all null values in 'Normalized Accumulated Consumption (L/day)' by diving the consumption by the number of meters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predicted_values = dataset_1_act_eco_filtered['Accumulated Consumption (L/day)'] / dataset_1_act_eco_filtered['Number of meters']\n",
        "\n",
        "dataset_1_act_eco_filtered.loc[dataset_1_act_eco_filtered['Normalized Accumulated Consumption (L/day)'].isnull(), 'Normalized Accumulated Consumption (L/day)'] = predicted_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Number of null values in Normalized Accumulated Consumption (L/day)\n",
        "print(\"Number of Nulls in the dataset:\", dataset_1_act_eco_filtered['Normalized Accumulated Consumption (L/day)'].isnull().sum())\n",
        "\n",
        "# Number of negatives values in Normalized Accumulated Consumption (L/day)\n",
        "print(\"Number of Negatives in the dataset:\", len(dataset_1_act_eco_filtered[dataset_1_act_eco_filtered['Normalized Accumulated Consumption (L/day)'] < 0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets visualize the Consumption after having cleaned and projected the incorrect values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PLOTS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbnBit8D3vw-"
      },
      "source": [
        "## 5. Analysis of anomalies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdOEgXxN3xZe"
      },
      "source": [
        "##### **5.1. Classification Criteria**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Previously, we stored all anomalies for later analysis. We are now going to state a classification criteria in order to classify said anomalies into 3 categories: leak or waste, system error or correct but misclassified.\n",
        "This classification is based on the average of the correct real data, I'm not using the predicted one by the models but it might not be very accurate. This should be done with specific data portraying leaks, waste and system errors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lgp8Ww9z3y_s"
      },
      "outputs": [],
      "source": [
        "# Count of Outliers and Negative values\n",
        "outliers_count = (anomalies['Normalized Accumulated Consumption (L/day)'] > 0).sum()\n",
        "negative_count = (anomalies['Normalized Accumulated Consumption (L/day)'] < 0).sum()\n",
        "\n",
        "total_count = len(anomalies)\n",
        "outliers_percentage = (outliers_count / total_count) * 100\n",
        "negative_percentage = (negative_count / total_count) * 100\n",
        "\n",
        "labels = ['Outliers', 'Negative Consumption']\n",
        "sizes = [outliers_percentage, negative_percentage]\n",
        "colors = ['cornflowerblue', 'lightskyblue']\n",
        "\n",
        "# Plotting the pie chart\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=140)\n",
        "plt.title('Percentage of Outliers and Negative Consumption')\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we compute the minimum, average and maximum values per type of Use so we can classify each values accordingly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# For each type of Use we can compute the min, average and max values\n",
        "grouped_data = clean_data.groupby('Use')['Normalized Accumulated Consumption (L/day)'].agg(['min', 'mean', 'max'])\n",
        "\n",
        "# Accessing min, average, and max values for a specific 'Use' type\n",
        "use_type = 'Domèstic/Doméstico/Domestic'\n",
        "domestic_min = grouped_data.loc[use_type, 'min']\n",
        "domestic_average = grouped_data.loc[use_type, 'mean']\n",
        "domestic_max = grouped_data.loc[use_type, 'max']\n",
        "\n",
        "use_type = 'Comercial/Comercial/Commercial'\n",
        "commercial_min = grouped_data.loc[use_type, 'min']\n",
        "commercial_average = grouped_data.loc[use_type, 'mean']\n",
        "commercial_max = grouped_data.loc[use_type, 'max']\n",
        "\n",
        "use_type = 'Industrial/Industrial/Industrial'\n",
        "industrial_min = grouped_data.loc[use_type, 'min']\n",
        "industrial_average = grouped_data.loc[use_type, 'mean']\n",
        "industrial_max = grouped_data.loc[use_type, 'max']\n",
        "\n",
        "grouped_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLVqrbfK3zJV"
      },
      "source": [
        "##### **5.2. Anomalies classification**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following function gets the anomalies dataset and depending on the type of Use applies a classification. I considered consumption values over the maximum value to be of type 'Leak or Waste'. If the consumption is negative then I consider this a 'Data Collection System Error' and in case of not belonging in any of these two groups then the value would be 'Correct but Misclassified'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def anomalies_classification(dataset_anomalies):\n",
        "    if dataset_anomalies['Use'] == 'Domèstic/Doméstico/Domestic':\n",
        "        if dataset_anomalies['Normalized Accumulated Consumption (L/day)'] > domestic_max:\n",
        "            return 'Leak or Waste'\n",
        "        elif dataset_anomalies['Normalized Accumulated Consumption (L/day)'] < 0:\n",
        "            return 'Data Collection System Error'\n",
        "        else:\n",
        "            return 'Correct but Misclassified'\n",
        "        \n",
        "    elif dataset_anomalies['Use'] == 'Comercial/Comercial/Commercial':\n",
        "        if dataset_anomalies['Normalized Accumulated Consumption (L/day)'] > commercial_max:\n",
        "            return 'Leak or Waste'\n",
        "        elif dataset_anomalies['Normalized Accumulated Consumption (L/day)'] < 0:\n",
        "            return 'Data Collection System Error'\n",
        "        else:\n",
        "            return 'Correct but Misclassified'\n",
        "        \n",
        "    elif dataset_anomalies['Use'] == 'Industrial/Industrial/Industrial':\n",
        "        if dataset_anomalies['Normalized Accumulated Consumption (L/day)'] > industrial_max:\n",
        "            return 'Leak or Waste'\n",
        "        elif dataset_anomalies['Normalized Accumulated Consumption (L/day)'] < 0:\n",
        "            return 'Data Collection System Error'\n",
        "        else:\n",
        "            return 'Correct but Misclassified'\n",
        "    \n",
        "\n",
        "anomalies['Classification'] = anomalies.apply(anomalies_classification, axis=1)\n",
        "anomalies.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can visualize the new classification in total and per type of Use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Count occurrences of each classification\n",
        "classification_counts = anomalies['Classification'].value_counts()\n",
        "labels = classification_counts.index\n",
        "sizes = classification_counts.values\n",
        "colors = ['lightskyblue', 'blue', 'cornflowerblue']\n",
        "\n",
        "# Plotting the pie chart\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=140)\n",
        "plt.title('Total Percentage of Each Anomaly Classification')\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Filter the DataFrame\n",
        "domestic_anomalies = anomalies[anomalies['Use'] == 'Domèstic/Doméstico/Domestic']\n",
        "\n",
        "# Count occurrences of each classification\n",
        "classification_counts = domestic_anomalies['Classification'].value_counts()\n",
        "labels = classification_counts.index\n",
        "sizes = classification_counts.values\n",
        "colors = ['lightskyblue', 'blue', 'cornflowerblue'] \n",
        "\n",
        "# Plotting the pie chart\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=140)\n",
        "plt.title('Percentage of Each Anomaly Classification for Domestic Use')\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Filter the DataFrame\n",
        "domestic_anomalies = anomalies[anomalies['Use'] == 'Comercial/Comercial/Commercial']\n",
        "\n",
        "# Count occurrences of each classification\n",
        "classification_counts = domestic_anomalies['Classification'].value_counts()\n",
        "labels = classification_counts.index\n",
        "sizes = classification_counts.values\n",
        "colors = ['lightskyblue', 'blue', 'cornflowerblue']\n",
        "\n",
        "# Plotting the pie chart\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=140)\n",
        "plt.title('Percentage of Each Anomaly Classification for Commercial Use')\n",
        "plt.axis('equal') \n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Filter the DataFrame\n",
        "domestic_anomalies = anomalies[anomalies['Use'] == 'Industrial/Industrial/Industrial']\n",
        "\n",
        "# Count occurrences of each classification\n",
        "classification_counts = domestic_anomalies['Classification'].value_counts()\n",
        "labels = classification_counts.index\n",
        "sizes = classification_counts.values\n",
        "colors = ['lightskyblue', 'blue', 'cornflowerblue']\n",
        "\n",
        "# Plotting the pie chart\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=140)\n",
        "plt.title('Percentage of Each Anomaly Classification for industrial Use')\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cmJ1YnOu3zw5"
      },
      "outputs": [],
      "source": [
        "# Download anomalies\n",
        "anomalies.to_csv('anomalies_dataset1_activitat_eco_v2.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bS3XgWUx31uo"
      },
      "source": [
        "## 6. Export improved data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nHtwvWSS32VD"
      },
      "outputs": [],
      "source": [
        "dataset_1_act_eco_filtered.to_csv('updated_dataset_1_activitat_eco_v2.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
