{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOf5R8F7nmrp"
      },
      "source": [
        "## Improving Water Consumption Management in Barcelona through Data Quality Enhancement and Prediction Models\n",
        "#### **TFG 2023-2024**\n",
        "#### **Author: Edith Ruiz Macià**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReOttuSdF7xM"
      },
      "source": [
        "### Analysis of dataset Dataset1_v2.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lGWCNxTDDrQ"
      },
      "source": [
        "### **About the Notebook**\n",
        "\n",
        "The aim of this notebook (.ipynb) is to analyze and modify the dataset dataset1_v2.csv provided by Aigües de Barcelona in order to improve the quality of the data and learn its insights through its visualization.\n",
        "\n",
        "The code is structured in different levels.\n",
        "\n",
        "1. Given the database, we start with an Exploratory Data Analysis that will provide us a general knowledge of how the raw data looks like.\n",
        "2. Then, an intensive data cleaning will be applied which will transform the data and handle null, negative, and outlier values.\n",
        "3. We will enhance the data by adding columns that will add up to the understanding of the data and also, by combining outside data that might be relevant to ours.\n",
        "4. In order to have a complete dataset we will make a projection of absent or incorrect values previously detected in section 2. To do so, we will test several algorithms and see with which one we obtain better results.\n",
        "5. Anomalies that were previously discarded and substituted by projected values with our predictive model must also be taken into account and analyzed. That is why we will classify and visualize them.\n",
        "\n",
        "Finally, the updated resulting database can be downloaded in order to be used in other programs or other purposes of the user."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ry3WPdzDFDq5"
      },
      "source": [
        "#### ***Execution time***\n",
        "bla bla bla\n",
        "\n",
        "#### ***How to execute the code***\n",
        "This is a Python Notebook, so the code should be run either cell by cell or by running all the notebook at once. At the beginning, review and make sure the paths to all datasets are correct for your folders' distribution in order for everything to run smoothly.\n",
        "\n",
        "#### ***Libraries and packages used in the project***\n",
        "Here is a brief explanation of the imported libraries:\n",
        "\n",
        "\n",
        "1. **math:** It is a Python standard library that provides basic math functions\n",
        "\n",
        "2. **datetime:** It is also part of a standard library and is used to work with dates and times\n",
        "\n",
        "3. **collections:** Another standard library that provides specialized data types like Counter which is used to count elements in a list\n",
        "\n",
        "3. **numpy:** Fundamental library for numerical computation. Specifically, linalg is a linear algebra module within NumPy\n",
        "\n",
        "3. **time:** Standard library for measuring time and working with dates and times\n",
        "\n",
        "4. **re:** Regular expression library used to search and manipulate text strings based on a given pattern\n",
        "\n",
        "5. **json:** Standard library for working with JSON (JavaScript Object Notation) format\n",
        "\n",
        "6. **pandas:** Very popular library for data manipulation and analysis. It is used to work with data structures such as DataFrames\n",
        "\n",
        "7. **matplotlib.pyplot:** Used to create graphs and visualizations\n",
        "\n",
        "8. **sklearn.model_selection:** Provides tools to split data sets into training and test subsets for a model\n",
        "\n",
        "9. **sklearn.linear_model**: Contains implementations of linear regression models\n",
        "\n",
        "10. **sklearn.metrics:** Contains metrics for evaluating machine learning models\n",
        "\n",
        "11. **sklearn.preprocessing:** Provides functions to preprocess data before fitting a model\n",
        "\n",
        "12. **sklearn.compose:** Contains utilities for building and combining transformers\n",
        "\n",
        "13. **sklearn.feature_extraction.text:** Tools for extracting features from text data\n",
        "\n",
        "14. **sklearn.cluster:** Contains clustering algorithms such as KMeans\n",
        "\n",
        "15. **sklearn.metrics.pairwise:** Metrics to calculate similarity between samples\n",
        "\n",
        "16. **seaborn:** Data visualization library based on Matplotlib. It provides a high-level interface for drawing attractive statistical graphs\n",
        "\n",
        "17. **xgboost:** Library to implement Gradient Boosting models\n",
        "\n",
        "18. **sklearn.ensemble:** Contains implementations of ensemble models, such as Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# In case you are working in google colab you can mount your google drive account here\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfzK-W6wnph9"
      },
      "source": [
        "## 0. Importing datasets and libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install shap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-zrD8OnHnvb9"
      },
      "outputs": [],
      "source": [
        "# Import the necessary libraries\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import xgboost as xgb\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.svm import SVR\n",
        "import shap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "aL3GMhCanz1D",
        "outputId": "a22db5ed-1c50-4dea-b3b4-7d53671d1210"
      },
      "outputs": [],
      "source": [
        "# Change your path of the data if necessary\n",
        "dataset1 = 'C:/Users/edith/Desktop/TFG/Datasets/Aigües de Barcelona/Sets-de-dades/dataset1_v2.csv'\n",
        "dataset1 = pd.read_csv(dataset1)\n",
        "dataset1.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUVAzDzzoYct"
      },
      "source": [
        "## 1. Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hOsADlioZyE"
      },
      "source": [
        "As a starting point, we need to explore the data. We analyze the summary of descriptive statistics and plots in order to detect the corresponding anomalies and subsequently process them correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yad59Cc6obJ9",
        "outputId": "06f0a634-41cf-44fc-d8a7-f52f0b7ac591"
      },
      "outputs": [],
      "source": [
        "dataset1.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "id": "kZgvaDzro4Pr",
        "outputId": "d207a079-1546-461d-b57a-01809f6e7f51"
      },
      "outputs": [],
      "source": [
        "dataset1.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "aeTP4Cotd1h4",
        "outputId": "6713de65-5cc8-4eeb-e8b0-77d2ac5de471"
      },
      "outputs": [],
      "source": [
        "# Accumulated Consumption over Time\n",
        "dataset1['Data/Fecha/Date'] = pd.to_datetime(dataset1['Data/Fecha/Date'])\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(dataset1['Data/Fecha/Date'], dataset1['Consum acumulat (L/dia)/Consumo acumulado(L/día)/Accumulated Consumption (L/day)'], color='blue', marker='o', s=50, alpha=0.7, label='Data Points')\n",
        "plt.xlabel('Date', fontsize=12)\n",
        "plt.ylabel('Accumulated Consumption (L/day)', fontsize=12)\n",
        "plt.title('Time Series Plot of Accumulated Consumption', fontsize=15)\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "X0gwK0yhgR1v",
        "outputId": "4b418540-fdfb-4ee8-d983-4de879689a65"
      },
      "outputs": [],
      "source": [
        "# Aggregate total consumption for each municipality\n",
        "total_consumption_per_municipality = dataset1.groupby('Municipi/Municipio/Municipality')['Consum acumulat (L/dia)/Consumo acumulado(L/día)/Accumulated Consumption (L/day)'].sum()\n",
        "total_consumption_per_municipality.plot(kind='bar', figsize=(10,6))\n",
        "plt.xlabel('Municipality', fontsize=12)\n",
        "plt.ylabel('Total Accumulated Consumption (L/day)', fontsize=12)\n",
        "plt.title('Total Accumulated Consumption per Municipality', fontsize=15)\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Poscodes of Barcelona:\")\n",
        "print(dataset1[dataset1['Municipi/Municipio/Municipality'] == 'BARCELONA'][\"Codi postal/Código postal/Postcode\"].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "waxZNJw9gt9W",
        "outputId": "846b0408-8699-4fc6-9f7f-f614cb44c91f"
      },
      "outputs": [],
      "source": [
        "# Aggregate total consumption for each type of use\n",
        "total_consumption_per_use = dataset1.groupby('Ús/Uso/Use')['Consum acumulat (L/dia)/Consumo acumulado(L/día)/Accumulated Consumption (L/day)'].sum()\n",
        "plt.figure(figsize=(10, 6))\n",
        "total_consumption_per_use.plot(kind='pie', autopct='%1.1f%%', startangle=140, colors=['lightskyblue', 'blue', 'cornflowerblue'])\n",
        "plt.title('Total Accumulated Consumption per Type of Use', fontsize=15)\n",
        "plt.ylabel(None)\n",
        "plt.tight_layout()\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_myG9a8Go7x2"
      },
      "source": [
        "**Insights:**\n",
        "- Negative consumption values are considered as incorrect values for the Cumulative Consumption (L/day) column. Consumption has to be positive.\n",
        "- The number of meters ('Number of meters') varies greatly, with some locations having just one meter and others up to 1254 meters.\n",
        "- In the scatter plot we can clearly see outliers that will be removed later on.\n",
        "- Most of the consumption is accumulated in the Municipality of Barcelona and it's of Domestic use."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NHZoyyho_B9"
      },
      "source": [
        "## 2. Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kh2CE04-pAEc"
      },
      "source": [
        "##### **2.1. Data Transformation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VN1_wGorpBp8"
      },
      "outputs": [],
      "source": [
        "# Rename dataset1_v2 columns\n",
        "new_column_names = {\n",
        "    'Secció Censal/Sección censal/Census section': 'Census section',\n",
        "    'Districte/Distrito/District': 'District',\n",
        "    'Codi postal/Código postal/Postcode': 'Postcode',\n",
        "    'Municipi/Municipio/Municipality': 'Municipality',\n",
        "    'Data/Fecha/Date': 'Date',\n",
        "    'Ús/Uso/Use': 'Use',\n",
        "    'Nombre de comptadors/Número de contadores/Number of meters': 'Number of meters',\n",
        "    'Consum acumulat (L/dia)/Consumo acumulado(L/día)/Accumulated Consumption (L/day)': 'Accumulated Consumption (L/day)'\n",
        "}\n",
        "\n",
        "dataset1.rename(columns=new_column_names, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Randomly sample 700000 data points\n",
        "sampled_indices = np.random.choice(len(dataset1), size=700000, replace=False)\n",
        "sampled_data = dataset1.iloc[sampled_indices]\n",
        "\n",
        "# Scatter Plot for Accumulated Consumption vs Number of Meters without negatives\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(sampled_data['Number of meters'], sampled_data['Accumulated Consumption (L/day)'], color='blue', marker='o', s=50, alpha=0.7, label='Data Points')\n",
        "plt.title('Accumulated Consumption vs Number of Meters without negatives', fontsize=15)\n",
        "plt.xlabel('Number of Meters', fontsize=12)\n",
        "plt.ylabel('Accumulated Consumption (L/day)', fontsize=12)\n",
        "plt.ylim(0, 100000)\n",
        "\n",
        "# Calculate the trend line\n",
        "x = sampled_data['Number of meters']\n",
        "y = sampled_data['Accumulated Consumption (L/day)']\n",
        "m, b = np.polyfit(x, y, 1)\n",
        "plt.plot(x, m*x + b, color='red')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We cannot assure that consumption is not normalized, but according to the plots above it does not look like it therefore, they have to be normalized. We will divide the Accumulated Consumption by the Number of Meters in order to obtain the Accumulated Consumption for a single Meter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Normalization of Accumulated Consumption (L/day) by Number of meters\n",
        "dataset1['Normalized Accumulated Consumption (L/day)'] = round(dataset1['Accumulated Consumption (L/day)'] / dataset1['Number of meters'], 3)\n",
        "dataset1.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Distribution of normalized vs non normalized\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.scatter(dataset1[\"Date\"], dataset1[\"Accumulated Consumption (L/day)\"], color='blue',  marker='o', s=20, alpha=0.7, label='Accumulated Consumption')\n",
        "plt.scatter(dataset1[\"Date\"], dataset1[\"Normalized Accumulated Consumption (L/day)\"], color='lightskyblue',  marker='o', s=20, alpha=0.7, label='Normalized Accumulated Consumption')\n",
        "plt.title(\"Distribution of Accumulated Consumption vs Normalized Accumulated Consumption over Time\", fontsize=12)\n",
        "plt.xlabel(\"Date\", fontsize=10)\n",
        "plt.ylabel(\"Accumulated Consumption (L/day)\", fontsize=10)\n",
        "plt.xticks(rotation=45, ha='right', fontsize=8)\n",
        "plt.yticks(fontsize=8)\n",
        "plt.grid(True)\n",
        "plt.legend(fontsize=7)\n",
        "tick_frequency = 100\n",
        "plt.gca().xaxis.set_major_locator(mdates.DayLocator(interval=tick_frequency))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "plt.scatter(dataset1[\"Date\"], dataset1[\"Normalized Accumulated Consumption (L/day)\"], color='blue',  marker='o', s=20, alpha=0.7, label='Normalized Accumulated Consumption')\n",
        "plt.title(\"Distribution of Accumulated Consumption vs Normalized Accumulated Consumption over Time\", fontsize=12)\n",
        "plt.xlabel(\"Date\", fontsize=10)\n",
        "plt.ylabel(\"Accumulated Consumption (L/day)\", fontsize=10)\n",
        "plt.xticks(rotation=45, ha='right', fontsize=8)\n",
        "plt.yticks(fontsize=8)\n",
        "plt.grid(True)\n",
        "plt.legend(fontsize=7)\n",
        "tick_frequency = 100\n",
        "plt.gca().xaxis.set_major_locator(mdates.DayLocator(interval=tick_frequency))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset1_box = dataset1.dropna(subset = ['District'])\n",
        "dataset1_box = dataset1_box.dropna(subset = ['Census section'])\n",
        "\n",
        "use_cases = dataset1_box['Use'].unique()\n",
        "num_plots = len(use_cases)\n",
        "num_columns = 3\n",
        "num_rows = int(np.ceil(num_plots / num_columns))\n",
        "\n",
        "fig, axes = plt.subplots(num_rows, num_columns, figsize=(15, num_rows * 7))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, use_case in enumerate(use_cases):\n",
        "    iteration_dataset = dataset1_box[dataset1_box['Use'] == use_case].copy()\n",
        "    columns_to_plot = ['Normalized Accumulated Consumption (L/day)']\n",
        "    axes[i].boxplot([iteration_dataset[col] for col in columns_to_plot], labels=columns_to_plot)\n",
        "    axes[i].set_title(f'Boxplot of {use_case}', fontsize=12)\n",
        "    axes[i].set_xlabel('Columns', fontsize=10)\n",
        "    axes[i].set_ylabel('Values', fontsize=10)\n",
        "    axes[i].tick_params(axis='x', labelsize=8)\n",
        "    axes[i].tick_params(axis='y', labelsize=8)\n",
        "\n",
        "# Hide any unused subplots if the number of use cases is not a multiple of num_columns\n",
        "for j in range(i + 1, num_rows * num_columns):\n",
        "    fig.delaxes(axes[j])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7iSkrixEpGqk",
        "outputId": "2f2e27fe-4d5c-4888-c554-d8373e3f24af"
      },
      "outputs": [],
      "source": [
        "dataset1.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sb66ZGRpJ0v"
      },
      "source": [
        "##### **2.2. Null Values**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MerBwvTQpKWG",
        "outputId": "62a6886e-6096-4b3f-94dd-1caaddce35ea"
      },
      "outputs": [],
      "source": [
        "# Null values per column\n",
        "columns_dataset1 = dataset1.columns\n",
        "for column in columns_dataset1:\n",
        "  print(\"Column:\", column, \"- Null values: \", dataset1[column].isnull().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bmSPPUVpR-G"
      },
      "source": [
        "##### **2.3. Wrong Values**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fAfqhPypfpf"
      },
      "source": [
        "Now we want to identify the erroneous values in our data. We consider as wrong values the negative consumptions. We will store those values for later analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-lLBigGpecM",
        "outputId": "943695fd-28b1-443c-9414-5ce8ea43a0c0"
      },
      "outputs": [],
      "source": [
        "# We identify negative values in Normalized Accumulated Consumption (L/day)\n",
        "num_negative_consum_rows = len(dataset1[dataset1['Normalized Accumulated Consumption (L/day)'] < 0])\n",
        "print(\"Negative in 'Normalized Accumulated Consumption (L/day)' in the dataset:\", num_negative_consum_rows)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "sFfGUPtG69mz",
        "outputId": "8fc48bac-8c07-4274-e5ec-d0e7337cb900"
      },
      "outputs": [],
      "source": [
        "# Copy the negative values into the anomalies dataset for later analysis\n",
        "anomalies = dataset1[dataset1['Normalized Accumulated Consumption (L/day)'] < 0].copy()\n",
        "anomalies.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oNxPoH7c0VO-",
        "outputId": "bae3d5ea-60a7-4bed-b1a2-0aa74e0b7df1"
      },
      "outputs": [],
      "source": [
        "# Replace negative values in Normalized Accumulated Consumption (L/day) by null\n",
        "dataset1_1 = dataset1.copy()\n",
        "dataset1_1.loc[dataset1_1['Normalized Accumulated Consumption (L/day)'] < 0, 'Normalized Accumulated Consumption (L/day)'] = np.nan\n",
        "\n",
        "# Let's put null also the values of the Accumulated Consumption (L/day)\n",
        "dataset1_1.loc[dataset1_1['Accumulated Consumption (L/day)'] < 0, 'Accumulated Consumption (L/day)'] = np.nan\n",
        "\n",
        "# Number of negative values in Normalized Accumulated Consumption (L/day) after removing negative values\n",
        "num_negative_consum_rows2 = len(dataset1_1[dataset1_1['Normalized Accumulated Consumption (L/day)'] < 0])\n",
        "print(\"Negative in 'Normalized Accumulated Consumption (L/day)' in the dataset:\", num_negative_consum_rows2)\n",
        "\n",
        "# Number of null values in Normalized Accumulated Consumption (L/day) after removing negative values\n",
        "print(\"Number of Nulls in the dataset without negative values:\", dataset1_1['Normalized Accumulated Consumption (L/day)'].isnull().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSjHxTNopeBy"
      },
      "source": [
        "##### **2.4. Outliers**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XVAelZCQpSVt"
      },
      "outputs": [],
      "source": [
        "# Treatment of outliers in Normalized Accumulated Consumption (L/day) with the IQR method\n",
        "\n",
        "# We separate according to USE: Industrial, Domestic or Commercial\n",
        "domestic_df = dataset1_1[dataset1_1['Use'].str.contains('Domestic', case=False, na=False)].copy()\n",
        "industrial_df = dataset1_1[dataset1_1['Use'].str.contains('Industrial', case=False, na=False)].copy()\n",
        "comercial_df = dataset1_1[dataset1_1['Use'].str.contains('Commercial', case=False, na=False)].copy()\n",
        "\n",
        "def outliers_iqr(dataframe):\n",
        "    global anomalies\n",
        "\n",
        "    # We compute the IQR for the 'Normalized Accumulated Consumption (L/day)' column\n",
        "    consum_col = 'Normalized Accumulated Consumption (L/day)'\n",
        "    Q1 = dataframe[consum_col].quantile(0.25)\n",
        "    Q3 = dataframe[consum_col].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "\n",
        "    # We identify the outliers\n",
        "    outlier_filter = ((dataframe[consum_col] < (Q1 - 1.5 * IQR)) | (dataframe[consum_col] > (Q3 + 1.5 * IQR)))\n",
        "\n",
        "    # Copy the outlier value into the anomalies dataset for later analysis\n",
        "    outliers = dataframe[outlier_filter].copy()\n",
        "    anomalies = pd.concat([anomalies, outliers], ignore_index=True)\n",
        "\n",
        "    # And replace them with null\n",
        "    dataframe.loc[outlier_filter, consum_col] = np.nan\n",
        "\n",
        "    return dataframe\n",
        "\n",
        "# We apply the function for each dataframe corresponding to each \"Use\"\n",
        "domestic_df = outliers_iqr(domestic_df)\n",
        "industrial_df = outliers_iqr(industrial_df)\n",
        "comercial_df = outliers_iqr(comercial_df)\n",
        "\n",
        "dataset1_filtered = pd.concat([domestic_df, industrial_df, comercial_df], ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let's put to null also the values that correspond to those outliers in 'Accumulated Consumption (L/day)'\n",
        "dataset1_filtered.loc[dataset1_filtered['Normalized Accumulated Consumption (L/day)'].isnull(), 'Accumulated Consumption (L/day)'] = np.nan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s4Rzp6PJ76xd",
        "outputId": "286cd111-f4c0-45fc-9d9f-235f299875df"
      },
      "outputs": [],
      "source": [
        "anomalies.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FgdORjoQpThR",
        "outputId": "b58056e0-9be4-4bdf-d653-c2110a6fa42b"
      },
      "outputs": [],
      "source": [
        "# Number of null values in Normalized Accumulated Consumption (L/day) prior to removing outliers\n",
        "print(\"Number of Nulls in the dataset with outliers but no negatives:\", dataset1_1['Normalized Accumulated Consumption (L/day)'].isnull().sum())        #dataset with outliers\n",
        "\n",
        "# Number of null values in Normalized Accumulated Consumption (L/day) after removing outliers\n",
        "print(\"Number of Nulls in the dataset without outliers nor negatives:\", dataset1_filtered['Normalized Accumulated Consumption (L/day)'].isnull().sum())   #dataset without outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset1_box = dataset1_filtered.dropna(subset = ['District'])\n",
        "dataset1_box = dataset1_box.dropna(subset = ['Census section'])\n",
        "dataset1_box = dataset1_box.dropna(subset = ['Accumulated Consumption (L/day)'])\n",
        "dataset1_box = dataset1_box.dropna(subset = ['Normalized Accumulated Consumption (L/day)'])\n",
        "\n",
        "use_cases = dataset1_box['Use'].unique()\n",
        "num_plots = len(use_cases)\n",
        "num_columns = 3\n",
        "num_rows = int(np.ceil(num_plots / num_columns))\n",
        "\n",
        "fig, axes = plt.subplots(num_rows, num_columns, figsize=(15, num_rows * 7))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, use_case in enumerate(use_cases):\n",
        "    iteration_dataset = dataset1_box[dataset1_box['Use'] == use_case].copy()\n",
        "    columns_to_plot = ['Normalized Accumulated Consumption (L/day)']\n",
        "    axes[i].boxplot([iteration_dataset[col] for col in columns_to_plot], labels=columns_to_plot)\n",
        "    axes[i].set_title(f'Boxplot of {use_case}', fontsize=12)\n",
        "    axes[i].set_xlabel('Columns', fontsize=10)\n",
        "    axes[i].set_ylabel('Values', fontsize=10)\n",
        "    axes[i].tick_params(axis='x', labelsize=8)\n",
        "    axes[i].tick_params(axis='y', labelsize=8)\n",
        "\n",
        "# Hide any unused subplots if the number of use cases is not a multiple of num_columns\n",
        "for j in range(i + 1, num_rows * num_columns):\n",
        "    fig.delaxes(axes[j])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAmXMoZ5p8bI"
      },
      "source": [
        "##### **2.5. Storage of Anomalies**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sML2FXI_gvXg"
      },
      "source": [
        "If all anomalies in the data have been cleaned correctly we should have the sum of the number of outlier values and negatives remaining in our anomalies stored.\n",
        "\n",
        "Negative values removed = 6222\n",
        "\n",
        "Outliers removed = 334898 - 6222 = 328676\n",
        "\n",
        "Total number of anomalies = 334898"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BzCZgymEp8zq"
      },
      "outputs": [],
      "source": [
        "# Save current clean dataset for classification of anomalies\n",
        "clean_data = dataset1_filtered.copy()\n",
        "\n",
        "anomalies.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9hKPXtiqGeJ"
      },
      "source": [
        "## 3. Data Enhancement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YiFQH-csqHUu",
        "outputId": "f708967a-a78f-4f75-9252-1b8e9a88de0b"
      },
      "outputs": [],
      "source": [
        "# Convert \"Date\" column to datetime\n",
        "dataset1_filtered['Date'] = pd.to_datetime(dataset1_filtered['Date'])\n",
        "dataset1_filtered.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "vqQQmsEPrPtD",
        "outputId": "a7283d7c-cb79-4a42-eb18-e901aeb67a3d"
      },
      "outputs": [],
      "source": [
        "# We create a new \"Season\" column based on the \"Date\" column\n",
        "def map_to_season(month):\n",
        "    if 3 <= month <= 5:\n",
        "        return 'Spring'\n",
        "    elif 6 <= month <= 8:\n",
        "        return 'Summer'\n",
        "    elif 9 <= month <= 11:\n",
        "        return 'Autumn'\n",
        "    else:\n",
        "        return 'Winter'\n",
        "\n",
        "dataset1_filtered['Season'] = dataset1_filtered['Date'].dt.month.map(map_to_season)\n",
        "\n",
        "# We create a new column \"Day_of_Week\" based on the column \"Date\"\n",
        "dataset1_filtered['Day of Week'] = dataset1_filtered['Date'].dt.day_name()\n",
        "\n",
        "# We create a new column \"Month\" based on the column \"Date\"\n",
        "dataset1_filtered['Month'] = dataset1_filtered['Date'].dt.month\n",
        "\n",
        "# We create a new column \"Year\" based on the column \"Date\"\n",
        "dataset1_filtered['Year'] = dataset1_filtered['Date'].dt.year\n",
        "\n",
        "dataset1_filtered.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0mMjpSfoPxe"
      },
      "source": [
        "As we can see now all data in this dataset ranges from years 2019 to 2022. This is important as now we are going to add new information from other datasets and we are going to need the dates to match."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zwYkk53tsTIA",
        "outputId": "6a133416-70f9-4064-b636-d1fc0740e084"
      },
      "outputs": [],
      "source": [
        "dataset1_filtered['Year'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "temperature = 'C:/Users/edith/Desktop/TFG\\Datasets/Dades meteorològiques de la XEMA/Temperature_Dades_Meteorologiques_XEMA.csv'\n",
        "temperature = pd.read_csv(temperature)\n",
        "temperature['DATA_LECTURA'] = pd.to_datetime(temperature['DATA_LECTURA'])\n",
        "temperature['DATA_LECTURA'] = temperature['DATA_LECTURA'].dt.date\n",
        "temperature['DATA_LECTURA'] = pd.to_datetime(temperature['DATA_LECTURA'])\n",
        "temperature = temperature.groupby(['CODI_ESTACIO', 'DATA_LECTURA'])['VALOR_LECTURA'].mean().reset_index()\n",
        "temperature.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rel_humidity = 'C:/Users/edith/Desktop/TFG\\Datasets/Dades meteorològiques de la XEMA/Relative_Humidity_Dades_Meteorologiques_XEMA.csv'\n",
        "rel_humidity = pd.read_csv(rel_humidity)\n",
        "rel_humidity['DATA_LECTURA'] = pd.to_datetime(rel_humidity['DATA_LECTURA'])\n",
        "rel_humidity['DATA_LECTURA'] = rel_humidity['DATA_LECTURA'].dt.date\n",
        "rel_humidity = rel_humidity.groupby(['CODI_ESTACIO', 'DATA_LECTURA'])['VALOR_LECTURA'].mean().reset_index()\n",
        "rel_humidity.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "atm_pressure = 'C:/Users/edith/Desktop/TFG\\Datasets/Dades meteorològiques de la XEMA/Atmospheric_Pressure_Dades_Meteorologiques_XEMA.csv'\n",
        "atm_pressure = pd.read_csv(atm_pressure)\n",
        "atm_pressure['DATA_LECTURA'] = pd.to_datetime(atm_pressure['DATA_LECTURA'])\n",
        "atm_pressure['DATA_LECTURA'] = atm_pressure['DATA_LECTURA'].dt.date\n",
        "atm_pressure = atm_pressure.groupby(['CODI_ESTACIO', 'DATA_LECTURA'])['VALOR_LECTURA'].mean().reset_index()\n",
        "atm_pressure.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "precipitation = 'C:/Users/edith/Desktop/TFG\\Datasets/Dades meteorològiques de la XEMA/Precipitation_Dades_Meteorologiques_XEMA.csv'\n",
        "precipitation = pd.read_csv(precipitation)\n",
        "precipitation['DATA_LECTURA'] = pd.to_datetime(precipitation['DATA_LECTURA'])\n",
        "precipitation['DATA_LECTURA'] = precipitation['DATA_LECTURA'].dt.date\n",
        "precipitation = precipitation.groupby(['CODI_ESTACIO', 'DATA_LECTURA'])['VALOR_LECTURA'].mean().reset_index()\n",
        "precipitation.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "solar_rad = 'C:/Users/edith/Desktop/TFG\\Datasets/Dades meteorològiques de la XEMA/Solar_Radation_global_Dades_Meteorologiques_XEMA.csv'\n",
        "solar_rad = pd.read_csv(solar_rad)\n",
        "solar_rad['DATA_LECTURA'] = pd.to_datetime(solar_rad['DATA_LECTURA'])\n",
        "solar_rad['DATA_LECTURA'] = solar_rad['DATA_LECTURA'].dt.date\n",
        "solar_rad = solar_rad.groupby(['CODI_ESTACIO', 'DATA_LECTURA'])['VALOR_LECTURA'].mean().reset_index()\n",
        "solar_rad.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "postalcode_to_station = {\n",
        "    8001: 'X4', 8002: 'X4', 8003: 'X2', 8004: 'X4', 8005: 'X2', 8006: 'D5', 8007: 'X4', 8008: 'X4', 8009: 'X4', 8010: 'X4',\n",
        "    8011: 'X4', 8012: 'D5', 8013: 'X2', 8014: 'XL', 8015: 'X4', 8016: 'D5', 8017: 'X8', 8018: 'X2', 8019: 'X2', 8020: 'X2',\n",
        "    8021: 'X8', 8022: 'D5', 8023: 'D5', 8024: 'D5', 8025: 'D5', 8026: 'X2', 8027: 'D5', 8028: 'XL', 8029: 'X8', 8030: 'D5',\n",
        "    8031: 'D5', 8032: 'D5', 8033: 'D5', 8034: 'X8', 8035: 'D5', 8036: 'X4', 8037: 'X4', 8038: 'X4', 8039: 'X4', 8040: 'XL',\n",
        "    8041: 'D5', 8042: 'D5'\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def mapping_geotemporal(dataset, geotemporal, attribute):\n",
        "\n",
        "    # if dataset['Date'].dtype == 'datetime64[ns]':\n",
        "    #     if any(dataset['Date'].dt.time != pd.Timestamp(0).time()):\n",
        "    #         dataset[\"Date\"] = dataset[\"Date\"].dt.date\n",
        "\n",
        "    for index, row in dataset.iterrows():\n",
        "        municipality = row[\"Municipality\"]\n",
        "        date = row[\"Date\"]\n",
        "        postcode = row[\"Postcode\"]\n",
        "\n",
        "        if municipality == \"GAVA\":\n",
        "            filtered_row = geotemporal[(geotemporal[\"CODI_ESTACIO\"] == \"UG\") & (geotemporal[\"DATA_LECTURA\"] == date)]\n",
        "            if not filtered_row.empty:\n",
        "                dataset.at[index, attribute] = filtered_row.iloc[0][\"VALOR_LECTURA\"]\n",
        "        \n",
        "        elif municipality == \"VILADECANS\":\n",
        "            filtered_row = geotemporal[(geotemporal[\"CODI_ESTACIO\"] == \"UG\") & (geotemporal[\"DATA_LECTURA\"] == date)]\n",
        "            if not filtered_row.empty:\n",
        "                dataset.at[index, attribute] = filtered_row.iloc[0][\"VALOR_LECTURA\"]\n",
        "        \n",
        "        elif municipality == \"SANT ADRIA\":\n",
        "            filtered_row = geotemporal[(geotemporal[\"CODI_ESTACIO\"] == \"WU\") & (geotemporal[\"DATA_LECTURA\"] == date)]\n",
        "            if not filtered_row.empty:\n",
        "                dataset.at[index, attribute] = filtered_row.iloc[0][\"VALOR_LECTURA\"]\n",
        "        \n",
        "        elif municipality == \"L'HOSPITALET LLOBR.\":\n",
        "            filtered_row = geotemporal[(geotemporal[\"CODI_ESTACIO\"] == \"XL\") & (geotemporal[\"DATA_LECTURA\"] == date)]\n",
        "            if not filtered_row.empty:\n",
        "                dataset.at[index, attribute] = filtered_row.iloc[0][\"VALOR_LECTURA\"]\n",
        "        \n",
        "        elif municipality == \"SANT FELIU LL.\":\n",
        "            filtered_row = geotemporal[(geotemporal[\"CODI_ESTACIO\"] == \"X8\") & (geotemporal[\"DATA_LECTURA\"] == date)]\n",
        "            if not filtered_row.empty:\n",
        "                dataset.at[index, attribute] = filtered_row.iloc[0][\"VALOR_LECTURA\"]\n",
        "                        \n",
        "        elif municipality == \"BARCELONA\":\n",
        "            station_code = postalcode_to_station.get(postcode)\n",
        "            filtered_row = geotemporal[(geotemporal[\"CODI_ESTACIO\"] == station_code) & (geotemporal[\"DATA_LECTURA\"] == date)]\n",
        "            if not filtered_row.empty:\n",
        "                dataset.at[index, attribute] = filtered_row.iloc[0][\"VALOR_LECTURA\"]\n",
        "\n",
        "\n",
        "mapping_geotemporal(dataset1_filtered, temperature, \"Temperature\")\n",
        "dataset1_filtered"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXp271bIoc1k"
      },
      "source": [
        "Lets add information to the dataset about the precipitations in Barcelona for the dates of each entrie as well as information about the temperatures. Both precipitation and temperatures should have a strong correlation with the consumed water."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "-7_DciUdodP6",
        "outputId": "26f7bc88-d1eb-47e2-a588-a5a1a72a9ed9"
      },
      "outputs": [],
      "source": [
        "# Change your path of the data if necessary\n",
        "dataset_rain = 'C:/Users/edith/Desktop/TFG/Datasets/OpenDataBCN/precipitacionsbcndesde1786_2023_long.csv'\n",
        "dataset_rain = pd.read_csv(dataset_rain)\n",
        "dataset_rain.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "iaOLvh7npKEH",
        "outputId": "5350a191-12cc-48e0-a786-8a1226f9bf00"
      },
      "outputs": [],
      "source": [
        "# Filter the dataset to keep only the desired years\n",
        "years_to_keep = [2019, 2020, 2021, 2022]\n",
        "dataset_rain = dataset_rain[dataset_rain['Any'].isin(years_to_keep)]\n",
        "\n",
        "# Display the first few rows of the filtered dataset\n",
        "dataset_rain.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def map_to_precipitation(dataset_filtered, dataset_rain):\n",
        "\n",
        "    # Merge datasets on 'Year' and 'Month'\n",
        "    merged_data = pd.merge(dataset_filtered, dataset_rain, left_on=['Year', 'Month'], right_on=['Any', 'Mes'], how='left')\n",
        "\n",
        "    # Drop unnecessary columns from the merged dataset\n",
        "    merged_data.drop(['Any', 'Mes', 'Desc_Mes'], axis=1, inplace=True)\n",
        "\n",
        "    return merged_data\n",
        "\n",
        "dataset1_filtered = map_to_precipitation(dataset1_filtered, dataset_rain)\n",
        "dataset1_filtered.rename(columns = {'Precipitacions': 'Precipitations'}, inplace=True)\n",
        "dataset1_filtered.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Precipitation Over Time\n",
        "df_sorted = dataset1_filtered.sort_values(by='Date')\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(df_sorted['Date'], df_sorted['Precipitations'], color='blue', linewidth=2)\n",
        "plt.xlabel('Date', fontsize=10)\n",
        "plt.ylabel('Precipitations (mm)', fontsize=10)\n",
        "plt.title('Precipitations Over Time', fontsize=12)\n",
        "plt.xticks(fontsize=8)\n",
        "plt.yticks(fontsize=8)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "RnyeaiLzpBsh",
        "outputId": "518bdc7b-50b4-42eb-bc2e-577e757cd306"
      },
      "outputs": [],
      "source": [
        "# Change your path of the data if necessary\n",
        "dataset_temp = 'C:/Users/edith/Desktop/TFG/Datasets/OpenDataBCN/temperaturesbcndesde1780_2023_long.csv'\n",
        "dataset_temp = pd.read_csv(dataset_temp)\n",
        "dataset_temp.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "efIRgTbNqyLx",
        "outputId": "cd7f2888-b55c-4926-d4f0-f843397a163a"
      },
      "outputs": [],
      "source": [
        "# Filter the dataset to keep only the desired years\n",
        "years_to_keep = [2019, 2020, 2021, 2022]\n",
        "dataset_temp = dataset_temp[dataset_temp['Any'].isin(years_to_keep)]\n",
        "\n",
        "# Display the first few rows of the filtered dataset\n",
        "dataset_temp.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "PIF7mQJyruCl",
        "outputId": "d4458918-b79e-497b-a432-60bf68a359d2"
      },
      "outputs": [],
      "source": [
        "def map_to_temperature(dataset_filtered, dataset_temp):\n",
        "\n",
        "    # Merge datasets on 'Year' and 'Month'\n",
        "    merged_data = pd.merge(dataset_filtered, dataset_temp, left_on=['Year', 'Month'], right_on=['Any', 'Mes'], how='left')\n",
        "\n",
        "    # Drop unnecessary columns from the merged dataset\n",
        "    merged_data.drop(['Any', 'Mes', 'Desc_Mes'], axis=1, inplace=True)\n",
        "\n",
        "    return merged_data\n",
        "\n",
        "dataset1_filtered = map_to_temperature(dataset1_filtered, dataset_temp)\n",
        "dataset1_filtered.rename(columns = {'Temperatura': 'Temperature'}, inplace=True)\n",
        "dataset1_filtered.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Temperature Over Time\n",
        "df_sorted = dataset1_filtered.sort_values(by='Date')\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(df_sorted['Date'], df_sorted['Temperature'], color='blue', linewidth=2)\n",
        "plt.xlabel('Date', fontsize=10)\n",
        "plt.ylabel('Temperature (ºC)', fontsize=10)\n",
        "plt.title('Temperature Over Time', fontsize=12)\n",
        "plt.xticks(fontsize=8)\n",
        "plt.yticks(fontsize=8)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpQpa-Pyr5Gb"
      },
      "source": [
        "## 4. Data Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLLQkFabr5aU"
      },
      "source": [
        "In order to do the projection of incorrect values (negative and outliers) that right now happen to be null values in the dataset, we will implement several data prediction models, compare their results and conclude which gives the best results for our data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgBBP0Hhr7Hy"
      },
      "source": [
        "We will now encode the variables that aren't numerical in order to work with them and plot a correlation matrix to see which features explain best our target variable, Accumulated Consumption (L/day). We are making a prediction with Accumulated Consumption (L/day) instead of the normalized data because it was the original one and shows more promising results. At the end we just have to divide by the Number of Meters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pwYsQvzhr9KC"
      },
      "outputs": [],
      "source": [
        "# Convert \"Date\" column to object for label encoding\n",
        "dataset1_filtered['Date'] = dataset1_filtered['Date'].astype(object)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZcijQV_sBNH"
      },
      "outputs": [],
      "source": [
        "# Label Encoder\n",
        "\n",
        "# Initialize LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Apply LabelEncoder to each column containing categorical data\n",
        "for column in dataset1_filtered.columns:\n",
        "    if dataset1_filtered[column].dtype == 'object':\n",
        "        dataset1_filtered[column] = label_encoder.fit_transform(dataset1_filtered[column])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 772
        },
        "id": "0EYTTVk2sFrR",
        "outputId": "36cc7203-3af6-4de4-bcfc-9ca984a8af18"
      },
      "outputs": [],
      "source": [
        "# We remove the null values to see the correct correlation of the data\n",
        "dataset1_filtered = dataset1_filtered.dropna(subset = ['District'])\n",
        "dataset1_filtered = dataset1_filtered.dropna(subset = ['Census section'])\n",
        "\n",
        "dataset1_filtered_not_null = dataset1_filtered[~dataset1_filtered['Accumulated Consumption (L/day)'].isnull()]\n",
        "corr = dataset1_filtered_not_null.corr()\n",
        "plt.figure(figsize=(15,6))\n",
        "heatmap = sns.heatmap(corr, vmin=-1, vmax=1, annot=True, cmap='coolwarm')\n",
        "heatmap.set_title('Correlation Heatmap', fontdict={'fontsize': 15})\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SdeLODVIsP2o",
        "outputId": "7e121626-28f2-4b94-e3ee-1d3a05867b0f"
      },
      "outputs": [],
      "source": [
        "# Model Training\n",
        "\n",
        "# We remove the null values in the consumption column to train the model\n",
        "dataset1_filtered2 = dataset1_filtered.dropna(subset=['Accumulated Consumption (L/day)'])\n",
        "\n",
        "# Independent variables \n",
        "features = ['Use', 'Number of meters', 'Date', 'Postcode', 'Census section']\n",
        "\n",
        "X = dataset1_filtered2[features]\n",
        "\n",
        "# Target variable (dependent variable)\n",
        "y = dataset1_filtered2['Accumulated Consumption (L/day)']\n",
        "\n",
        "# 80% training and 20% testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# We make sure that X and Y have the same length for both train and test\n",
        "print(\"Lenght X_train:\", len(X_train))\n",
        "print(\"Length y_train:\", len(y_train))\n",
        "print(\"Lenght X_test:\", len(X_test))\n",
        "print(\"Length y_test:\", len(y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSOmzhZwFjGp"
      },
      "source": [
        "##### **4.1. Linear Regression**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMNYkzO2s-Jd",
        "outputId": "1f64e37f-fe94-4b16-8603-794e53aa66c2"
      },
      "outputs": [],
      "source": [
        "# Linear Regression\n",
        "lr_model = LinearRegression()\n",
        "lr_model.fit(X_train, y_train)\n",
        "y_pred = lr_model.predict(X_test)\n",
        "\n",
        "# Evalute the model\n",
        "mse_lr = mean_squared_error(y_test, y_pred)\n",
        "mae_lr = mean_absolute_error(y_test, y_pred)\n",
        "r2_lr = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Linear Regression Mean Squared Error (MSE): {round(np.sqrt(mse_lr), 4)}\")\n",
        "print(f\"Linear Regression Mean Absolute Error (MAE): {round(mae_lr, 4)}\")\n",
        "print(f\"Linear Regression R-squared (R2): {round(r2_lr, 4)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QogWBTzrtMHW"
      },
      "source": [
        "##### **4.2. Random Forest**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bIYI7d9GtPKl"
      },
      "outputs": [],
      "source": [
        "# Random Forest\n",
        "rf_model = RandomForestRegressor(random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
        "mae_rf = mean_absolute_error(y_test, y_pred_rf)\n",
        "r2_rf = r2_score(y_test, y_pred_rf)\n",
        "\n",
        "print(f\"Random Forest Mean Squared Error (MSE):\", round(np.sqrt(mse_rf), 4))\n",
        "print(f\"Random Forest Mean Absolute Error (MAE): {round(mae_rf, 4)}\")\n",
        "print(f\"Random Forest R-squared (R2):\", round(r2_rf, 4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5_VieyftUG9"
      },
      "source": [
        "##### **4.3. XGBoost**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9d69gEbGtWJp"
      },
      "outputs": [],
      "source": [
        "# XGBoost\n",
        "xgb_model = xgb.XGBRegressor(random_state=42)\n",
        "xgb_model.fit(X_train, y_train)\n",
        "y_pred_xgb = xgb_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
        "mae_xgb = mean_absolute_error(y_test, y_pred_xgb)\n",
        "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
        "\n",
        "print(f\"XGBoost Mean Squared Error (MSE):\", round(np.sqrt(mse_xgb), 4))\n",
        "print(f\"XGBoost Mean Absolute Error (MAE): {round(mae_xgb, 4)}\")\n",
        "print(f\"XGBoost R-squared (R2):\", round(r2_xgb, 4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be1fgGUx1Rc0"
      },
      "source": [
        "##### **4.4. K-Nearest Neighbors**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Range of k values to test\n",
        "k_values = range(1, 10)\n",
        "mse_values = []\n",
        "\n",
        "# Evaluation of the model for each k value\n",
        "for k in k_values:\n",
        "    knn_model = KNeighborsRegressor(n_neighbors=k)\n",
        "    knn_model.fit(X_train, y_train)\n",
        "    y_pred_knn = knn_model.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred_knn)\n",
        "    mse_values.append(mse)\n",
        "\n",
        "# Plot error rates vs k values\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(k_values, mse_values, marker='o', linestyle='dashed', color='b')\n",
        "plt.xlabel('Number of Neighbors K', fontsize=10)\n",
        "plt.ylabel('Mean Squared Error (MSE)', fontsize=10)\n",
        "plt.title('Error Rate vs. K Value', fontsize=12)\n",
        "plt.xticks(k_values)\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# k value with the minimum error rate\n",
        "best_k = k_values[np.argmin(mse_values)]\n",
        "print(f\"The best k value is: {best_k}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A50820AU1Q1H"
      },
      "outputs": [],
      "source": [
        "# K-Nearest Neighbors\n",
        "knn_model = KNeighborsRegressor(n_neighbors=2)\n",
        "knn_model.fit(X_train, y_train)\n",
        "y_pred_knn = knn_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse_knn = mean_squared_error(y_test, y_pred_knn)\n",
        "mae_knn = mean_absolute_error(y_test, y_pred_knn)\n",
        "r2_knn = r2_score(y_test, y_pred_knn)\n",
        "\n",
        "print(f\"K-Nearest Neighbors Mean Squared Error (MSE):\", round(np.sqrt(mse_knn), 4))\n",
        "print(f\"K-Nearest Neighbors Mean Absolute Error (MAE): {round(mae_knn, 4)}\")\n",
        "print(f\"K-Nearest Neighbors R-squared (R2):\", round(r2_knn, 4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41IqC6bb1Y2r"
      },
      "source": [
        "##### **4.5. Support Vector Machine**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model Training\n",
        "\n",
        "# We remove the null values in the consumption column to train the model\n",
        "dataset1_filtered2 = dataset1_filtered.dropna(subset=['Accumulated Consumption (L/day)'])\n",
        "\n",
        "# Reduce the working dataset\n",
        "rows = len(dataset1_filtered2) // 10\n",
        "dataset1_filtered2 = dataset1_filtered2.iloc[:rows]\n",
        "\n",
        "# Independent variables\n",
        "features = ['Use', 'Number of meters', 'Date', 'Postcode', 'Census section']\n",
        "X = dataset1_filtered2[features]\n",
        "\n",
        "# Target variable (dependent variable)\n",
        "y = dataset1_filtered2['Accumulated Consumption (L/day)']\n",
        "\n",
        "# 80% training and 20% testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# We make sure that X and Y have the same length for both train and test\n",
        "print(\"Lenght X_train:\", len(X_train))\n",
        "print(\"Length y_train:\", len(y_train))\n",
        "print(\"Lenght X_test:\", len(X_test))\n",
        "print(\"Length y_test:\", len(y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Support Vector Machine\n",
        "svm_model = SVR()\n",
        "svm_model.fit(X_train, y_train)\n",
        "y_pred_svm = svm_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse_svm = mean_squared_error(y_test, y_pred_svm)\n",
        "mae_svm = mean_absolute_error(y_test, y_pred_svm)\n",
        "r2_svm = r2_score(y_test, y_pred_svm)\n",
        "\n",
        "print(f\"Support Vector Machine Mean Squared Error (MSE):\", round(np.sqrt(mse_svm), 4))\n",
        "print(f\"Support Vector Machine Mean Absolute Error (MAE): {round(mae_svm, 4)}\")\n",
        "print(f\"Support Vector Machine R-squared (R2):\", round(r2_svm, 4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### **4.6. Explainability**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert the data to numpy arrays\n",
        "X_train_array = X_train.values\n",
        "X_test_array = X_test.values\n",
        "\n",
        "# We use a small subset of the training data to initialize the SHAP KernelExplainer\n",
        "background = shap.kmeans(X_train_array, 10)\n",
        "explainer = shap.KernelExplainer(knn_model.predict, background)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# We compute SHAP values for the test set\n",
        "shap_values = explainer.shap_values(X_test_array)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert shap_values to Explanation objects\n",
        "shap_values_exp = shap.Explanation(values=shap_values, base_values=explainer.expected_value, data=X_test_array, feature_names=features)\n",
        "\n",
        "# Visualize SHAP values\n",
        "shap.initjs()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "shap.summary_plot(shap_values, X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "features = ['Use', 'Number of meters', 'Date', 'Postcode', 'Census section']\n",
        "\n",
        "# Plot layout\n",
        "fig, axs = plt.subplots(2, 3, figsize=(20, 10))  # 2 rows, 3 columns\n",
        "axs = axs.flatten()\n",
        "\n",
        "# Dependence plots\n",
        "for i, ft in enumerate(features):\n",
        "    shap.dependence_plot(ft, shap_values, X_test, ax=axs[i], show=False)\n",
        "\n",
        "# Hide the empty subplot (the 6th plot space)\n",
        "fig.delaxes(axs[-1])\n",
        "\n",
        "# Adjust layout\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# For a single prediction\n",
        "shap.force_plot(explainer.expected_value, shap_values[0], X_test.iloc[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "shap.waterfall_plot(shap_values_exp[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "shap.decision_plot(shap_values_exp[0].base_values, shap_values_exp[0].values, X_test.iloc[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "features = ['Use', 'Number of meters', 'Date', 'Postcode', 'Census section']\n",
        "\n",
        "# Plot layout\n",
        "fig, axs = plt.subplots(2, 3, figsize=(20, 10))  # 2 rows, 3 columns\n",
        "axs = axs.flatten()\n",
        "\n",
        "# Scatter plots\n",
        "for i, ft in enumerate(features):\n",
        "    shap.plots.scatter(shap_values_exp[:, ft], ax=axs[i], show=False)\n",
        "\n",
        "# Hide the empty subplot (the 6th plot space)\n",
        "fig.delaxes(axs[-1])\n",
        "\n",
        "# Adjust layout\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### **4.7. Conclusions on Data Prediction Algorithms**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Based on the R-squared values obtained per each model I conclude that the K-Nearest Neighbors model, with 0.9705, is the most accurate for dataset1_v2.csv data. It is considered a good model if the R-squared value is higher than 0.9. Therefore now we will replace the missing values with the predictions of this model. We have already trained model so now we replace all null values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Identify the rows that have a NUll values in 'Accumulated Consumption (L/day)'\n",
        "missing_rows = dataset1_filtered[dataset1_filtered['Accumulated Consumption (L/day)'].isnull()]\n",
        "\n",
        "features2 = ['Use', 'Number of meters', 'Date', 'Postcode', 'Census section']\n",
        "X_ = missing_rows[features2]\n",
        "\n",
        "# Predict the values with the model\n",
        "predicted_values = knn_model.predict(X_)\n",
        "\n",
        "dataset1_filtered.loc[missing_rows.index, 'Accumulated Consumption (L/day)'] = predicted_values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now check that no null values nor negatives are in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Number of null values in Accumulated Consumption (L/day)\n",
        "print(\"Number of Nulls in the dataset:\", dataset1_filtered['Accumulated Consumption (L/day)'].isnull().sum())\n",
        "\n",
        "# Number of negatives values in Accumulated Consumption (L/day)\n",
        "print(\"Number of Negatives in the dataset:\", len(dataset1_filtered[dataset1_filtered['Accumulated Consumption (L/day)'] < 0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In order to keep working with our normalized values lets once again put all null values in 'Normalized Accumulated Consumption (L/day)' by diving the consumption by the number of meters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predicted_values = dataset1_filtered['Accumulated Consumption (L/day)'] / dataset1_filtered['Number of meters']\n",
        "\n",
        "dataset1_filtered.loc[dataset1_filtered['Normalized Accumulated Consumption (L/day)'].isnull(), 'Normalized Accumulated Consumption (L/day)'] = predicted_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Number of null values in Normalized Accumulated Consumption (L/day)\n",
        "print(\"Number of Nulls in the dataset:\", dataset1_filtered['Normalized Accumulated Consumption (L/day)'].isnull().sum())\n",
        "\n",
        "# Number of negatives values in Normalized Accumulated Consumption (L/day)\n",
        "print(\"Number of Negatives in the dataset:\", len(dataset1_filtered[dataset1_filtered['Normalized Accumulated Consumption (L/day)'] < 0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets visualize the Consumption after having cleaned and projected the incorrect values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PLOTS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUpUhTiDtbll"
      },
      "source": [
        "## 5. Analysis of anomalies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYzqF2KEtlIG"
      },
      "source": [
        "##### **5.1. Classification Criteria**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Previously, we stored all anomalies for later analysis. We are now going to state a classification criteria in order to classify said anomalies into 3 categories: leak or waste, system error or correct but misclassified.\n",
        "This classification is based on the average of the correct real data, I'm not using the predicted one by the models but it might not be very accurate. This should be done with specific data portraying leaks, waste and system errors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# % of anomalies with respect to the original dataset\n",
        "percentage = (334898 / 4820790 ) * 100\n",
        "print(\"% of anomalies with respect to the original dataset: \", round(percentage, 2), \"%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WECM3V0dtcLO"
      },
      "outputs": [],
      "source": [
        "# Count of Outliers and Negative values\n",
        "outliers_count = (anomalies['Normalized Accumulated Consumption (L/day)'] > 0).sum()\n",
        "negative_count = (anomalies['Normalized Accumulated Consumption (L/day)'] < 0).sum()\n",
        "\n",
        "total_count = len(anomalies)\n",
        "outliers_percentage = (outliers_count / total_count) * 100\n",
        "negative_percentage = (negative_count / total_count) * 100\n",
        "\n",
        "labels = ['Outliers', 'Negative Consumption']\n",
        "sizes = [outliers_percentage, negative_percentage]\n",
        "colors = ['cornflowerblue', 'lightskyblue']\n",
        "\n",
        "# Plotting the pie chart\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=140)\n",
        "plt.title('Percentage of Outliers and Negative Consumption')\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we compute the minimum, average and maximum values per type of Use so we can classify each values accordingly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# For each type of Use we can compute the min, average and max values\n",
        "grouped_data = clean_data.groupby('Use')['Normalized Accumulated Consumption (L/day)'].agg(['min', 'mean', 'max'])\n",
        "\n",
        "# Accessing min, average, and max values for a specific 'Use' type\n",
        "use_type = 'Domèstic/Doméstico/Domestic'\n",
        "domestic_min = grouped_data.loc[use_type, 'min']\n",
        "domestic_average = grouped_data.loc[use_type, 'mean']\n",
        "domestic_max = grouped_data.loc[use_type, 'max']\n",
        "\n",
        "use_type = 'Comercial/Comercial/Commercial'\n",
        "commercial_min = grouped_data.loc[use_type, 'min']\n",
        "commercial_average = grouped_data.loc[use_type, 'mean']\n",
        "commercial_max = grouped_data.loc[use_type, 'max']\n",
        "\n",
        "use_type = 'Industrial/Industrial/Industrial'\n",
        "industrial_min = grouped_data.loc[use_type, 'min']\n",
        "industrial_average = grouped_data.loc[use_type, 'mean']\n",
        "industrial_max = grouped_data.loc[use_type, 'max']\n",
        "\n",
        "grouped_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzpyN8pwtm1g"
      },
      "source": [
        "##### **5.2. Anomalies classification**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following function gets the anomalies dataset and depending on the type of Use applies a classification. I considered consumption values over the maximum value to be of type 'Leak or Waste'. If the consumption is negative then I consider this a 'Data Collection System Error' and in case of not belonging in any of these two groups then the value would be 'Correct but Misclassified'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YOItRBSxtouD"
      },
      "outputs": [],
      "source": [
        "def anomalies_classification(dataset_anomalies):\n",
        "    if dataset_anomalies['Use'] == 'Domèstic/Doméstico/Domestic':\n",
        "        if dataset_anomalies['Normalized Accumulated Consumption (L/day)'] > domestic_max:\n",
        "            return 'Leak or Waste'\n",
        "        elif dataset_anomalies['Normalized Accumulated Consumption (L/day)'] < 0:\n",
        "            return 'Data Collection System Error'\n",
        "        else:\n",
        "            return 'Correct but Misclassified'\n",
        "        \n",
        "    elif dataset_anomalies['Use'] == 'Comercial/Comercial/Commercial':\n",
        "        if dataset_anomalies['Normalized Accumulated Consumption (L/day)'] > commercial_max:\n",
        "            return 'Leak or Waste'\n",
        "        elif dataset_anomalies['Normalized Accumulated Consumption (L/day)'] < 0:\n",
        "            return 'Data Collection System Error'\n",
        "        else:\n",
        "            return 'Correct but Misclassified'\n",
        "        \n",
        "    elif dataset_anomalies['Use'] == 'Industrial/Industrial/Industrial':\n",
        "        if dataset_anomalies['Normalized Accumulated Consumption (L/day)'] > industrial_max:\n",
        "            return 'Leak or Waste'\n",
        "        elif dataset_anomalies['Normalized Accumulated Consumption (L/day)'] < 0:\n",
        "            return 'Data Collection System Error'\n",
        "        else:\n",
        "            return 'Correct but Misclassified'\n",
        "    \n",
        "\n",
        "anomalies['Classification'] = anomalies.apply(anomalies_classification, axis=1)\n",
        "anomalies.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can visualize the new classification in total and per type of Use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Count occurrences of each classification\n",
        "classification_counts = anomalies['Classification'].value_counts()\n",
        "labels = classification_counts.index\n",
        "sizes = classification_counts.values\n",
        "colors = ['lightskyblue', 'blue', 'cornflowerblue']\n",
        "\n",
        "# Plotting the pie chart\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=140)\n",
        "plt.title('Anomalies Classification')\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Filter the DataFrame\n",
        "domestic_anomalies = anomalies[anomalies['Use'] == 'Domèstic/Doméstico/Domestic']\n",
        "\n",
        "# Count occurrences of each classification\n",
        "classification_counts = domestic_anomalies['Classification'].value_counts()\n",
        "labels = classification_counts.index\n",
        "sizes = classification_counts.values\n",
        "colors = ['lightskyblue', 'blue', 'cornflowerblue'] \n",
        "\n",
        "# Plotting the pie chart\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=140)\n",
        "plt.title('Anomalies Classification for Domestic Use')\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Filter the DataFrame\n",
        "domestic_anomalies = anomalies[anomalies['Use'] == 'Comercial/Comercial/Commercial']\n",
        "\n",
        "# Count occurrences of each classification\n",
        "classification_counts = domestic_anomalies['Classification'].value_counts()\n",
        "labels = classification_counts.index\n",
        "sizes = classification_counts.values\n",
        "colors = ['lightskyblue', 'blue', 'cornflowerblue']\n",
        "\n",
        "# Plotting the pie chart\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=140)\n",
        "plt.title('Percentage of Each Anomaly Classification for Commercial Use')\n",
        "plt.axis('equal') \n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Filter the DataFrame\n",
        "domestic_anomalies = anomalies[anomalies['Use'] == 'Industrial/Industrial/Industrial']\n",
        "\n",
        "# Count occurrences of each classification\n",
        "classification_counts = domestic_anomalies['Classification'].value_counts()\n",
        "labels = classification_counts.index\n",
        "sizes = classification_counts.values\n",
        "colors = ['lightskyblue', 'blue', 'cornflowerblue']\n",
        "\n",
        "# Plotting the pie chart\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=140)\n",
        "plt.title('Percentage of Each Anomaly Classification for Industrial Use')\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download anomalies\n",
        "anomalies.to_csv('anomalies_dataset1_v2.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IT4KsnudtruM"
      },
      "source": [
        "## 6. Export improved data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jKjIXoR6ts8T"
      },
      "outputs": [],
      "source": [
        "dataset1_filtered.to_csv('updated_dataset1_v2.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "RpQpa-Pyr5Gb",
        "eUpUhTiDtbll",
        "IT4KsnudtruM"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
