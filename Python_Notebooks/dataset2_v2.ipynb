{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0nFBex7EUrw"
      },
      "source": [
        "## Improving Water Consumption Management in Barcelona through Data Quality Enhancement and Prediction Models\n",
        "#### **TFG 2023-2024**\n",
        "#### **Author: Edith Ruiz Macià**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnTWv_jjGkeU"
      },
      "source": [
        "### Analysis of dataset Dataset2_v2.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsY0SD9lDLqy"
      },
      "source": [
        "#### **About the Notebook**\n",
        "\n",
        "The aim of this notebook (.ipynb) is to analyze and modify the dataset dataset2_v2.csv provided by Aigües de Barcelona in order to improve the quality of the data and learn its insights through its visualization.\n",
        "\n",
        "The code is structured in different levels.\n",
        "\n",
        "1. Given the database, we start with an Exploratory Data Analysis that will provide us a general knowledge of how the raw data looks like.\n",
        "2. Then, an intensive data cleaning will be applied which will transform the data and handle null, negative, and outlier values.\n",
        "3. We will enhance the data by adding columns that will add up to the understanding of the data and also, by combining outside data that might be relevant to ours.\n",
        "4. In order to have a complete dataset we will make a projection of absent or incorrect values previously detected in section 2. To do so, we will test several algorithms and see with which one we obtain better results.\n",
        "5. Anomalies that were previously discarded and substituted by projected values with our predictive model must also be taken into account and analyzed. That is why we will classify and visualize them.\n",
        "\n",
        "Finally, the updated resulting database can be downloaded in order to be used in other programs or other purposes of the user."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRaef4mtFFaJ"
      },
      "source": [
        "#### ***Execution time***\n",
        "bla bla bla\n",
        "\n",
        "#### ***How to execute the code***\n",
        "This is a Python Notebook, so the code should be run either cell by cell or by running all the notebook at once. At the beginning, review and make sure the paths to all datasets are correct for your folders' distribution in order for everything to run smoothly.\n",
        "\n",
        "#### ***Libraries and packages used in the project***\n",
        "bla bla bla"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76R27y7VKPwx",
        "outputId": "807ddb30-2996-416b-97f8-ff7d325802ce"
      },
      "outputs": [],
      "source": [
        "# Mount your google drive account here\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhTE7LznKKpe"
      },
      "source": [
        "## 0. Importing datasets and libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2CMqlOYoHs57"
      },
      "outputs": [],
      "source": [
        "#import the necessary libraries\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import xgboost as xgb\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.svm import SVR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "VBO5XytnKwJu",
        "outputId": "8868ab25-a447-42cd-c2ae-6828a62f4351"
      },
      "outputs": [],
      "source": [
        "#Change your path of the data if necessary\n",
        "# dataset2 = '/content/drive/MyDrive/TFG/Datasets/Aigües de Barcelona/Dataset2_V2/dataset2_v2.csv'\n",
        "dataset2 = 'C:/Users/edith/Desktop/TFG/Datasets/Aigües de Barcelona/Dataset2_V2/dataset2_v2.csv'\n",
        "dataset2 = pd.read_csv(dataset2)\n",
        "dataset2.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pz8lKzLyK0ok"
      },
      "source": [
        "## 1. Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-N2aBkvVWTez"
      },
      "source": [
        "As a starting point, we need to explore the data. We analyze the summary of descriptive statistics and plots for each of the datasets in order to detect the corresponding anomalies and subsequently process them correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QnQf5Ny3K4yB",
        "outputId": "74d23b27-ab34-42bb-c413-1568adfd54c4"
      },
      "outputs": [],
      "source": [
        "dataset2.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "_aA8svCBK8gR",
        "outputId": "52550aff-5460-4de2-9859-98a1cfdc158b"
      },
      "outputs": [],
      "source": [
        "dataset2.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset2['Data/Fecha/Date'] = pd.to_datetime(dataset2['Data/Fecha/Date'])\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(dataset2['Data/Fecha/Date'], dataset2['Consum (L/h)/Consumo (L/h)/Consumption (L/h)'], color='blue', marker='o', s=50, alpha=0.7, label='Data Points')\n",
        "plt.xlabel('Date', fontsize=12)\n",
        "plt.ylabel('Consumption (L/h)', fontsize=12)\n",
        "plt.title('Time Series Plot of Consumption', fontsize=15)\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Aggregate total consumption for each type of use\n",
        "total_consumption_per_use = dataset2.groupby('Ús/Uso/Use')['Consum (L/h)/Consumo (L/h)/Consumption (L/h)'].sum()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "total_consumption_per_use.plot(kind='pie', autopct='%1.1f%%', startangle=140, colors=['lightskyblue', 'blue', 'cornflowerblue'])\n",
        "plt.title('Total Consumption per Type of Use', fontsize=14)\n",
        "plt.ylabel(None)\n",
        "plt.tight_layout()\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awKeelcoK_CN"
      },
      "source": [
        "**Insights:**\n",
        "- There are a significant number of zero values in Consumption. This can affect the performance of the model, especially in linear regression, since the presence of many zeros can affect the assumption of linearity.\n",
        "- Negative consumption values are considered as incorrect values for the Consumption (L/h) column. Consumption has to be positive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0D69uMqtLFeG"
      },
      "source": [
        "## 2. Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wv3RrGp0LOSw"
      },
      "source": [
        "##### **2.1. Data Transformation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gC74vUCYLEdO"
      },
      "outputs": [],
      "source": [
        "# Rename Dataset2 columns\n",
        "new_column_names = {\n",
        "    'ID': 'ID',\n",
        "    'Diàmetre (mm)/Diámetro (mm)/Diameter (mm)': 'Diameter (mm)',\n",
        "    'Ús/Uso/Use': 'Use',\n",
        "    \"Tipus d'activitat econòmica/Tipo de actividad económica/Type of economic activity\": 'Type of economic activity',\n",
        "    'Data/Fecha/Date': 'Date',\n",
        "    'Hora/Hora/Time': 'Time',\n",
        "    'Consum (L/h)/Consumo (L/h)/Consumption (L/h)': 'Consumption (L/h)'\n",
        "}\n",
        "\n",
        "dataset2.rename(columns=new_column_names, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6zVdll9wLKrd",
        "outputId": "3f74240b-2196-4ffa-c2b5-d3996ac3fe10"
      },
      "outputs": [],
      "source": [
        "dataset2.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-owQEFELUyq"
      },
      "source": [
        "##### **2.2. Null Values**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7XfNVNQLYHT",
        "outputId": "ad0be32b-24ff-49c5-a1b1-a3a56e87514e"
      },
      "outputs": [],
      "source": [
        "print(\"NULL VALUES PER COLUMN DATASET2\\n\")\n",
        "count = 0\n",
        "columns_dataset2 = dataset2.columns\n",
        "for column in columns_dataset2:\n",
        "  if dataset2[column].isnull().sum() != 0:\n",
        "    count = count + 1\n",
        "    print(\"Column:\", column, \"- Null values: \", dataset2[column].isnull().sum())\n",
        "\n",
        "if count == 0:\n",
        "  print(\"No columns with null values found\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### **2.3. Wrong Values**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we want to identify the erroneous values in our data. We consider as wrong values the negative consumptions. We will store those values for later analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# We identify negative values in consumption\n",
        "num_negative_consum_rows = len(dataset2[dataset2['Consumption (L/h)'] < 0])\n",
        "print(\"Negative in 'Consumption (L/h)' in the dataset:\", num_negative_consum_rows)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Copy the negative values into the anomalies dataset for later analysis\n",
        "anomalies = dataset2[dataset2['Consumption (L/h)'] < 0].copy()\n",
        "anomalies.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Replace negative values in Consumption (L/h) by null\n",
        "dataset2_1 = dataset2.copy()\n",
        "dataset2_1.loc[dataset2_1['Consumption (L/h)'] < 0, 'Consumption (L/h)'] = np.nan\n",
        "\n",
        "# Number of negative values in Consumption (L/h) after removing negative values\n",
        "num_negative_consum_rows2 = len(dataset2_1[dataset2_1['Consumption (L/h)'] < 0])\n",
        "print(\"Negative in 'Consumption (L/h)' in the dataset:\", num_negative_consum_rows2)\n",
        "\n",
        "# Number of null values in Consumption (L/h) after removing negative values\n",
        "print(\"Number of Nulls in the dataset without negative values:\", dataset2_1['Consumption (L/h)'].isnull().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjbMXGPAL-PX"
      },
      "source": [
        "##### **2.4. Outliers**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "NdySlnrSMAXx",
        "outputId": "7804b6fd-fc55-4ab7-e526-74fd3a55cfb9"
      },
      "outputs": [],
      "source": [
        "# Treatment of outliers in Consumption (L/h) with the IQR method\n",
        "\n",
        "# We separate according to USE: Industrial, Domestic or Commercial\n",
        "domestic_df = dataset2_1[dataset2_1['Use'].str.contains('Domèstic', case=False, na=False)].copy()\n",
        "industrial_df = dataset2_1[dataset2_1['Use'].str.contains('Industrial', case=False, na=False)].copy()\n",
        "comercial_df = dataset2_1[dataset2_1['Use'].str.contains('Comercial', case=False, na=False)].copy()\n",
        "\n",
        "def outliers_iqr(dataframe):\n",
        "    global anomalies\n",
        "\n",
        "    # We compute the IQR for the 'Consumption' column\n",
        "    consum_col = 'Consumption (L/h)'\n",
        "    Q1 = dataframe[consum_col].quantile(0.25)\n",
        "    Q3 = dataframe[consum_col].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "\n",
        "    # We identify the outliers\n",
        "    outlier_filter = ((dataframe[consum_col] < (Q1 - 1.5 * IQR)) | (dataframe[consum_col] > (Q3 + 1.5 * IQR)))\n",
        "    \n",
        "    # Copy the outlier value into the anomalies dataset for later analysis\n",
        "    outliers = dataframe[outlier_filter].copy()\n",
        "    anomalies = pd.concat([anomalies, outliers], ignore_index=True)\n",
        "    \n",
        "    # And replace them with null\n",
        "    dataframe.loc[outlier_filter, consum_col] = np.nan\n",
        "\n",
        "    return dataframe\n",
        "\n",
        "# We apply the function for each dataframe corresponding to each \"Use\"\n",
        "domestic_df = outliers_iqr(domestic_df)\n",
        "industrial_df = outliers_iqr(industrial_df)\n",
        "comercial_df = outliers_iqr(comercial_df)\n",
        "\n",
        "dataset2_filtered = pd.concat([domestic_df, industrial_df, comercial_df], ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "anomalies.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6gXBpMjMF1l",
        "outputId": "e7a46c26-72d7-4b6d-b017-b714e3998581"
      },
      "outputs": [],
      "source": [
        "# Number of null values in Consumption (L/h) prior to removing outliers\n",
        "print(\"Number of Nulls in the dataset with outliers but no negatives:\", dataset2_1['Consumption (L/h)'].isnull().sum())   #dataset with outliers\n",
        "\n",
        "# Number of null values in Consumption (L/h) after removing outliers\n",
        "print(\"Number of Nulls in the dataset without outliers nor negatives:\", dataset2_filtered['Consumption (L/h)'].isnull().sum()) #dataset without outliers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4o3NrxgQW0GU"
      },
      "source": [
        "##### **2.5. Storage of Anomalies**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If all anomalies in the data have been cleaned correctly we should have the sum of the number of outlier values and negatives remaining in our anomalies stored.\n",
        "\n",
        "Negative values removed = 567\n",
        "\n",
        "Outliers removed = 286754 - 567 = 286187\n",
        "\n",
        "Total number of anomalies = 286754"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j0BvIe7BlR4U"
      },
      "outputs": [],
      "source": [
        "# Save current clean dataset for classification of anomalies\n",
        "clean_data = dataset2_filtered.copy()\n",
        "\n",
        "anomalies.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SywrSlYWuQ7"
      },
      "source": [
        "## 3. Data Enhancement\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_C9VNXmLarj"
      },
      "outputs": [],
      "source": [
        "# Convert \"Date\" column to datetime\n",
        "dataset2_filtered['Date'] = pd.to_datetime(dataset2_filtered['Date'])\n",
        "dataset2_filtered.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "XsVY0k9FMwml",
        "outputId": "62b93987-7fc7-469c-cab0-dd57a83c092a"
      },
      "outputs": [],
      "source": [
        "# We create a new \"Season\" column based on the \"Date\" column\n",
        "def map_to_season(month):\n",
        "    if 3 <= month <= 5:\n",
        "        return 'Spring'\n",
        "    elif 6 <= month <= 8:\n",
        "        return 'Summer'\n",
        "    elif 9 <= month <= 11:\n",
        "        return 'Autumn'\n",
        "    else:\n",
        "        return 'Winter'\n",
        "\n",
        "dataset2_filtered['Season'] = dataset2_filtered['Date'].dt.month.map(map_to_season)\n",
        "\n",
        "# We create a new column \"Day_of_Week\" based on the column \"Date\"\n",
        "dataset2_filtered['Day of Week'] = dataset2_filtered['Date'].dt.day_name()\n",
        "\n",
        "# We create a new column \"Month\" based on the column \"Date\"\n",
        "dataset2_filtered['Month'] = dataset2_filtered['Date'].dt.month\n",
        "\n",
        "# We create a new column \"Year\" based on the column \"Date\"\n",
        "dataset2_filtered['Year'] = dataset2_filtered['Date'].dt.year\n",
        "\n",
        "dataset2_filtered.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we can see now all data in this dataset is from year 2022. This is important as now we are going to add new information from other datasets and we are going to need the dates to match."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yMOYLqg9tFjQ",
        "outputId": "66d313a1-7436-47d1-d4a6-602d8f27ad43"
      },
      "outputs": [],
      "source": [
        "dataset2_filtered['Year'].unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets add information to the dataset about the precipitations in Barcelona for the dates of each entrie as well as information about the temperatures. Both precipitation and temperatures should have a strong correlation with the consumed water."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Change your path of the data if necessary\n",
        "# dataset_rain = '/content/drive/MyDrive/TFG/Datasets/Geotemporals/precipitacionsbcndesde1786_2023_long.csv'\n",
        "dataset_rain = 'C:/Users/edith/Desktop/TFG/Datasets/Geotemporals/precipitacionsbcndesde1786_2023_long.csv'\n",
        "dataset_rain = pd.read_csv(dataset_rain)\n",
        "dataset_rain.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Filter the dataset to keep only the desired years\n",
        "years_to_keep = [2022]\n",
        "dataset_rain = dataset_rain[dataset_rain['Any'].isin(years_to_keep)]\n",
        "\n",
        "# Display the first few rows of the filtered dataset\n",
        "dataset_rain.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def map_to_precipitation(dataset_filtered, dataset_rain):\n",
        "\n",
        "    # Merge datasets on 'Year' and 'Month'\n",
        "    merged_data = pd.merge(dataset_filtered, dataset_rain, left_on=['Year', 'Month'], right_on=['Any', 'Mes'], how='left')\n",
        "\n",
        "    # Drop unnecessary columns from the merged dataset\n",
        "    merged_data.drop(['Any', 'Mes', 'Desc_Mes'], axis=1, inplace=True)\n",
        "\n",
        "    return merged_data\n",
        "\n",
        "dataset2_filtered = map_to_precipitation(dataset2_filtered, dataset_rain)\n",
        "dataset2_filtered.rename(columns = {'Precipitacions': 'Precipitations'}, inplace=True)\n",
        "dataset2_filtered.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Change your path of the data if necessary\n",
        "# dataset_temp = '/content/drive/MyDrive/TFG/Datasets/Geotemporals/temperaturesbcndesde1780_2023_long.csv'\n",
        "dataset_temp = 'C:/Users/edith/Desktop/TFG/Datasets/Geotemporals/temperaturesbcndesde1780_2023_long.csv'\n",
        "dataset_temp = pd.read_csv(dataset_temp)\n",
        "dataset_temp.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Filter the dataset to keep only the desired years\n",
        "years_to_keep = [2022]\n",
        "dataset_temp = dataset_temp[dataset_temp['Any'].isin(years_to_keep)]\n",
        "\n",
        "# Display the first few rows of the filtered dataset\n",
        "dataset_temp.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def map_to_temperature(dataset_filtered, dataset_temp):\n",
        "\n",
        "    # Merge datasets on 'Year' and 'Month'\n",
        "    merged_data = pd.merge(dataset_filtered, dataset_temp, left_on=['Year', 'Month'], right_on=['Any', 'Mes'], how='left')\n",
        "\n",
        "    # Drop unnecessary columns from the merged dataset\n",
        "    merged_data.drop(['Any', 'Mes', 'Desc_Mes'], axis=1, inplace=True)\n",
        "\n",
        "    return merged_data\n",
        "\n",
        "dataset2_filtered = map_to_temperature(dataset2_filtered, dataset_temp)\n",
        "dataset2_filtered.rename(columns = {'Temperatura': 'Temperature'}, inplace=True)\n",
        "dataset2_filtered.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEIlR1X9W7sq"
      },
      "source": [
        "## 4. Data Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5dJvRBYldLa"
      },
      "source": [
        "##### **4.1. Linear**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In order to do the projection of incorrect values (negative and outliers) that right now happen to be null values in the dataset, we will implement several data prediction models, compare their results and conclude which gives the best results for our data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will now encode the variables that aren't numerical in order to work with them and plot a correlation matrix to see which features explain best our target variable, Accumulated Consumption (L/day)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HzDUs7ynN4u6"
      },
      "outputs": [],
      "source": [
        "# Convert \"Date\" column to object for label encoding\n",
        "dataset2_filtered['Date'] = dataset2_filtered['Date'].astype(object)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7MkC6TTLv-N2"
      },
      "outputs": [],
      "source": [
        "# Label Encoder\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import pandas as pd\n",
        "\n",
        "# Initialize LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Apply LabelEncoder to each column containing categorical data\n",
        "for column in dataset2_filtered.columns:\n",
        "    if dataset2_filtered[column].dtype == 'object':\n",
        "        dataset2_filtered[column] = label_encoder.fit_transform(dataset2_filtered[column])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 708
        },
        "id": "rfPMeV1EqzLQ",
        "outputId": "79faca5e-328a-421f-bf7d-105ff3f8be20"
      },
      "outputs": [],
      "source": [
        "dataset2_filtered_not_null = dataset2_filtered[~dataset2_filtered['Consumption (L/h)'].isnull()]\n",
        "corr = dataset2_filtered_not_null.corr()\n",
        "plt.figure(figsize=(13,6))\n",
        "heatmap = sns.heatmap(corr, vmin=-1, vmax=1, annot=True, cmap='coolwarm')\n",
        "heatmap.set_title('Correlation Heatmap', fontdict={'fontsize': 14})\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8Lm5licrcKu",
        "outputId": "b4388415-9cc1-43aa-9abb-b06adff72275"
      },
      "outputs": [],
      "source": [
        "# Model Training\n",
        "\n",
        "# We remove the null values in the consumption column to train the model\n",
        "dataset2_filtered2 = dataset2_filtered.dropna(subset=['Consumption (L/h)'])\n",
        "\n",
        "# Independent variables\n",
        "features = ['Use', 'Time', 'Temperature', 'Precipitations', 'Season', 'Day of Week', 'Diameter (mm)', 'Type of economic activity', 'Date'] # 0.2791\n",
        "# features = ['Use', 'Time', 'Temperature', 'Precipitations', 'Season', 'Day of Week', 'Diameter (mm)', 'Type of economic activity'] # 0.2738\n",
        "# features = ['Use', 'Time', 'Temperature', 'Precipitations', 'Season', 'Day of Week', 'Type of economic activity'] # 0.2667\n",
        "# features = ['Use', 'Time','Precipitations', 'Season', 'Day of Week', 'Type of economic activity'] # 0.264\n",
        "# features = ['Use', 'Time','Season', 'Day of Week', 'Type of economic activity'] # 0.2613 \n",
        "\n",
        "\n",
        "X = dataset2_filtered2[features]\n",
        "\n",
        "# Target variable (dependent variable)\n",
        "y = dataset2_filtered2['Consumption (L/h)']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# We make sure that X and Y have the same length for both train and test\n",
        "print(\"Lenght X_train:\", len(X_train))\n",
        "print(\"Length y_train:\", len(y_train))\n",
        "print(\"Lenght X_test:\", len(X_test))\n",
        "print(\"Length y_test:\", len(y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### **4.1. Linear Regression**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "heIZVgkarlpZ",
        "outputId": "3d7acfeb-5828-4e5b-ea37-7c03437f0ce7"
      },
      "outputs": [],
      "source": [
        "# Linear Regression\n",
        "lr_model = LinearRegression()\n",
        "lr_model.fit(X_train, y_train)\n",
        "y_pred = lr_model.predict(X_test)\n",
        "\n",
        "# Evalute the model\n",
        "mse_lr = mean_squared_error(y_test, y_pred)\n",
        "mae_lr = mean_absolute_error(y_test, y_pred)\n",
        "r2_lr = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Mean Squared Error (MSE): {round(np.sqrt(mse_lr), 4)}\")\n",
        "print(f\"Mean Absolute Error (MAE): {round(mae_lr, 4)}\")\n",
        "print(f\"R-squared (R2): {round(r2_lr, 4)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFDDjeWYljmO"
      },
      "source": [
        "##### **4.2. Random Forest**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5838BhBlqz_N",
        "outputId": "078cba66-78ec-43ea-d294-8530510cfcfe"
      },
      "outputs": [],
      "source": [
        "# Random Forest\n",
        "rf_model = RandomForestRegressor(random_state=0)\n",
        "rf_model.fit(X_train, y_train)\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
        "mae_rf = mean_absolute_error(y_test, y_pred_rf)\n",
        "r2_rf = r2_score(y_test, y_pred_rf)\n",
        "\n",
        "print(f\"Random Forest Mean Squared Error (MSE):\", round(np.sqrt(mse_rf), 4))\n",
        "print(f\"Mean Absolute Error (MAE): {round(mae_rf, 4)}\")\n",
        "print(f\"Random Forest R-squared (R2):\", round(r2_rf, 4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWnhy2VAlmHn"
      },
      "source": [
        "##### **4.3. XGBoost**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cofgwaDbq0nr",
        "outputId": "2604dbe6-cfac-453c-bcac-0abeeaf526b0"
      },
      "outputs": [],
      "source": [
        "# XGBoost\n",
        "xgb_model = xgb.XGBRegressor(random_state=42)\n",
        "xgb_model.fit(X_train, y_train)\n",
        "y_pred_xgb = xgb_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
        "mae_xgb = mean_absolute_error(y_test, y_pred_xgb)\n",
        "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
        "\n",
        "print(f\"XGBoost Mean Squared Error (MSE):\", round(np.sqrt(mse_xgb), 4))\n",
        "print(f\"Mean Absolute Error (MAE): {round(mae_xgb, 4)}\")\n",
        "print(f\"XGBoost R-squared (R2):\", round(r2_xgb, 4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pu28uQbfloTu"
      },
      "source": [
        "##### **4.4. K-Nearest Neighbors**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# K-Nearest Neighbors\n",
        "knn_model = KNeighborsRegressor()\n",
        "knn_model.fit(X_train, y_train)\n",
        "y_pred_knn = knn_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse_knn = mean_squared_error(y_test, y_pred_knn)\n",
        "mae_knn = mean_absolute_error(y_test, y_pred_knn)\n",
        "r2_knn = r2_score(y_test, y_pred_knn)\n",
        "\n",
        "print(f\"K-Nearest Neighbors Mean Squared Error (MSE):\", round(np.sqrt(mse_knn), 4))\n",
        "print(f\"Mean Absolute Error (MAE): {round(mae_knn, 4)}\")\n",
        "print(f\"K-Nearest Neighbors R-squared (R2):\", round(r2_knn, 4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### **4.5. Support Vector Machine**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model Training\n",
        "\n",
        "# We remove the null values in the consumption column to train the model\n",
        "dataset2_filtered2 = dataset2_filtered.dropna(subset=['Consumption (L/h)'])\n",
        "\n",
        "# Reduce the working dataset\n",
        "rows = len(dataset2_filtered2) // 25\n",
        "dataset2_filtered2 = dataset2_filtered2.iloc[:rows]\n",
        "\n",
        "# Independent variables\n",
        "features = ['Use', 'Time', 'Temperature', 'Precipitations', 'Season', 'Day of Week', 'Diameter (mm)', 'Type of economic activity', 'Date'] \n",
        "X = dataset2_filtered2[features]\n",
        "\n",
        "# Target variable (dependent variable)\n",
        "y = dataset2_filtered2['Consumption (L/h)']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# We make sure that X and Y have the same length for both train and test\n",
        "print(\"Lenght X_train:\", len(X_train))\n",
        "print(\"Length y_train:\", len(y_train))\n",
        "print(\"Lenght X_test:\", len(X_test))\n",
        "print(\"Length y_test:\", len(y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Support Vector Machine\n",
        "svm_model = SVR()\n",
        "svm_model.fit(X_train, y_train)\n",
        "y_pred_svm = svm_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse_svm = mean_squared_error(y_test, y_pred_svm)\n",
        "mae_svm = mean_absolute_error(y_test, y_pred_svm)\n",
        "r2_svm = r2_score(y_test, y_pred_svm)\n",
        "\n",
        "print(f\"Support Vector Machine Mean Squared Error (MSE):\", round(np.sqrt(mse_svm), 4))\n",
        "print(f\"Mean Absolute Error (MAE): {round(mae_svm, 4)}\")\n",
        "print(f\"Support Vector Machine R-squared (R2):\", round(r2_svm, 4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJ-0bkCflrbq"
      },
      "source": [
        "##### **4.6. Conclusions on Data Prediction Algorithms**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Based on the R-squared values obtained per each model I conclude that 0.2791 is the least worse model for dataset1_v2.csv data. It is considered a good model if the R-squared value is higher than 0.9, therefore non of the aboce are good models. Anyway, we will replace the missing values with the predictions of this model. We have already trained model so now we replace all null values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cqk-P32Pq1YI"
      },
      "outputs": [],
      "source": [
        "# Identifiquem les files amb valor Null en 'Consumption (L/h)'\n",
        "missing_rows = dataset2_filtered[dataset2_filtered['Consumption (L/h)'].isnull()]\n",
        "\n",
        "features2 = ['Use', 'Time', 'Temperature', 'Precipitations', 'Season', 'Day of Week', 'Diameter (mm)', 'Type of economic activity', 'Date']\n",
        "X_ = missing_rows[features2]\n",
        "\n",
        "# Predim els valors amb el model\n",
        "predicted_values = xgb_model.predict(X_)\n",
        "\n",
        "dataset2_filtered.loc[missing_rows.index, 'Consumption (L/h)'] = predicted_values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now check that no null values nor negatives are in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Number of null values in Consumption (L/h)\n",
        "print(\"Number of Nulls in the dataset:\", dataset2_filtered['Consumption (L/h)'].isnull().sum())\n",
        "\n",
        "# Number of negatives values in Consumption (L/h)\n",
        "print(\"Number of Negatives in the dataset:\", len(dataset2_filtered[dataset2_filtered['Consumption (L/h)'] < 0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets visualize the Consumption after having cleaned and projected the incorrect values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PLOTS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKJ-8KlVl26Q"
      },
      "source": [
        "## 5. Analysis of anomalies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIDgl20rl8r1"
      },
      "source": [
        "##### **5.1. Classification Criteria**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Previously, we stored all anomalies for later analysis. We are now going to state a classification criteria in order to classify said anomalies into 3 categories: leak or waste, system error or correct but misclassified.\n",
        "This classification is based on the average of the correct real data, I'm not using the predicted one by the models but it might not be very accurate. This should be done with specific data portraying leaks, waste and system errors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2E3JcSoPmGXp"
      },
      "outputs": [],
      "source": [
        "# Count of Outliers and Negative values\n",
        "outliers_count = (anomalies['Consumption (L/h)'] > 0).sum()\n",
        "negative_count = (anomalies['Consumption (L/h)'] < 0).sum()\n",
        "\n",
        "total_count = len(anomalies)\n",
        "outliers_percentage = (outliers_count / total_count) * 100\n",
        "negative_percentage = (negative_count / total_count) * 100\n",
        "\n",
        "labels = ['Outliers', 'Negative Consumption']\n",
        "sizes = [outliers_percentage, negative_percentage]\n",
        "colors = ['cornflowerblue', 'lightskyblue']\n",
        "\n",
        "# Plotting the pie chart\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=140)\n",
        "plt.title('Percentage of Outliers and Negative Consumption')\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we compute the minimum, average and maximum values per type of Use so we can classify each values accordingly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# For each type of Use we can compute the min, average and max values\n",
        "grouped_data = clean_data.groupby('Use')['Consumption (L/h)'].agg(['min', 'mean', 'max'])\n",
        "\n",
        "# Accessing min, average, and max values for a specific 'Use' type\n",
        "use_type = 'Domèstic/Doméstico/Domestic'\n",
        "domestic_min = grouped_data.loc[use_type, 'min']\n",
        "domestic_average = grouped_data.loc[use_type, 'mean']\n",
        "domestic_max = grouped_data.loc[use_type, 'max']\n",
        "\n",
        "use_type = 'Comercial/Comercial/Commercial'\n",
        "commercial_min = grouped_data.loc[use_type, 'min']\n",
        "commercial_average = grouped_data.loc[use_type, 'mean']\n",
        "commercial_max = grouped_data.loc[use_type, 'max']\n",
        "\n",
        "use_type = 'Industrial/Industrial/Industrial'\n",
        "industrial_min = grouped_data.loc[use_type, 'min']\n",
        "industrial_average = grouped_data.loc[use_type, 'mean']\n",
        "industrial_max = grouped_data.loc[use_type, 'max']\n",
        "\n",
        "grouped_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqdlfcvbmCBZ"
      },
      "source": [
        "##### **5.2. Anomalies classification**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following function gets the anomalies dataset and depending on the type of Use applies a classification. I considered consumption values over the maximum value to be of type 'Leak or Waste'. If the consumption is negative then I consider this a 'Data Collection System Error' and in case of not belonging in any of these two groups then the value would be 'Correct but Misclassified'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def anomalies_classification(dataset_anomalies):\n",
        "    if dataset_anomalies['Use'] == 'Domèstic/Doméstico/Domestic':\n",
        "        if dataset_anomalies['Consumption (L/h)'] > domestic_max:\n",
        "            return 'Leak or Waste'\n",
        "        elif dataset_anomalies['Consumption (L/h)'] < 0:\n",
        "            return 'Data Collection System Error'\n",
        "        else:\n",
        "            return 'Correct but Misclassified'\n",
        "        \n",
        "    elif dataset_anomalies['Use'] == 'Comercial/Comercial/Commercial':\n",
        "        if dataset_anomalies['Consumption (L/h)'] > commercial_max:\n",
        "            return 'Leak or Waste'\n",
        "        elif dataset_anomalies['Consumption (L/h)'] < 0:\n",
        "            return 'Data Collection System Error'\n",
        "        else:\n",
        "            return 'Correct but Misclassified'\n",
        "        \n",
        "    elif dataset_anomalies['Use'] == 'Industrial/Industrial/Industrial':\n",
        "        if dataset_anomalies['Consumption (L/h)'] > industrial_max:\n",
        "            return 'Leak or Waste'\n",
        "        elif dataset_anomalies['Consumption (L/h)'] < 0:\n",
        "            return 'Data Collection System Error'\n",
        "        else:\n",
        "            return 'Correct but Misclassified'\n",
        "    \n",
        "\n",
        "anomalies['Classification'] = anomalies.apply(anomalies_classification, axis=1)\n",
        "anomalies.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can visualize the new classification in total and per type of Use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Count occurrences of each classification\n",
        "classification_counts = anomalies['Classification'].value_counts()\n",
        "labels = classification_counts.index\n",
        "sizes = classification_counts.values\n",
        "colors = ['lightskyblue', 'blue', 'cornflowerblue']\n",
        "\n",
        "# Plotting the pie chart\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=140)\n",
        "plt.title('Total Percentage of Each Anomaly Classification')\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Filter the DataFrame\n",
        "domestic_anomalies = anomalies[anomalies['Use'] == 'Domèstic/Doméstico/Domestic']\n",
        "\n",
        "# Count occurrences of each classification\n",
        "classification_counts = domestic_anomalies['Classification'].value_counts()\n",
        "labels = classification_counts.index\n",
        "sizes = classification_counts.values\n",
        "colors = ['lightskyblue', 'blue', 'cornflowerblue'] \n",
        "\n",
        "# Plotting the pie chart\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=140)\n",
        "plt.title('Percentage of Each Anomaly Classification for Domestic Use')\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Filter the DataFrame\n",
        "domestic_anomalies = anomalies[anomalies['Use'] == 'Comercial/Comercial/Commercial']\n",
        "\n",
        "# Count occurrences of each classification\n",
        "classification_counts = domestic_anomalies['Classification'].value_counts()\n",
        "labels = classification_counts.index\n",
        "sizes = classification_counts.values\n",
        "colors = ['lightskyblue', 'blue', 'cornflowerblue']\n",
        "\n",
        "# Plotting the pie chart\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=140)\n",
        "plt.title('Percentage of Each Anomaly Classification for Commercial Use')\n",
        "plt.axis('equal') \n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Filter the DataFrame\n",
        "domestic_anomalies = anomalies[anomalies['Use'] == 'Industrial/Industrial/Industrial']\n",
        "\n",
        "# Count occurrences of each classification\n",
        "classification_counts = domestic_anomalies['Classification'].value_counts()\n",
        "labels = classification_counts.index\n",
        "sizes = classification_counts.values\n",
        "colors = ['lightskyblue', 'blue', 'cornflowerblue']\n",
        "\n",
        "# Plotting the pie chart\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=140)\n",
        "plt.title('Percentage of Each Anomaly Classification for industrial Use')\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M5QACaGzW9D4"
      },
      "outputs": [],
      "source": [
        "# Download anomalies\n",
        "anomalies.to_csv('anomalies_dataset2_v2.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxWPCJltzR28"
      },
      "source": [
        "## 6. Export improved data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0qggrfOLzaCT"
      },
      "outputs": [],
      "source": [
        "dataset2_filtered.to_csv('updated_dataset2_v2.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "TJ-0bkCflrbq",
        "eKJ-8KlVl26Q",
        "nIDgl20rl8r1",
        "wqdlfcvbmCBZ"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
